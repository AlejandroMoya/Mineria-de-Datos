{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Práctica 3 - Multiclasificadores y selección de variables\n",
    "\n",
    "**Nombres:** \n",
    " - **Alejandro Moya Moya** - LinkedIn: https://www.linkedin.com/in/alejandro-moya-moya/\n",
    " - **Jorge Valero Molina** - LinkedIn: https://www.linkedin.com/in/jorge-valero-molina-0a2a0512b/\n",
    " \n",
    "**Año:** 2017/18\n",
    "\n",
    "**Revisión:** 0.8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Indice:\n",
    "* [1. Antes de empezar...](#1.-Antes-de-empezar...)\n",
    "* [2. Carga de los Datasets](#2.-Carga-de-los-Datasets)\n",
    "* [3. Uso y ajuste de métodos basados en ensembles](#3.-Uso-y-ajuste-de-métodos-basados-en-ensembles)\n",
    "    * [3.1 Implementación básica de un ensemble de manera manual](#3.1-Implementación-básica-de-un-ensemble-de-manera-manual)\n",
    "    * [3.2 Uso de Bagging, Boosting, Random Forest y Gradient Boosting](#3.2-Uso-de-Bagging,-Boosting,-Random-Forest-y-Gradient-Boosting)\n",
    "        * [3.2.1 Bagging](#3.2.1-Bagging)\n",
    "            * [3.2.1.1 Bagging del dataset Pima](#3.2.1.1-Bagging-del-dataset-Pima)\n",
    "            * [3.2.1.2 Bagging del dataset Wisconsin](#3.2.1.2-Bagging-del-dataset-Wisconsin)\n",
    "        * [3.2.2 Boosting](#3.2.2-Boosting)\n",
    "            * [3.2.2.1 Boosting del dataset Pima](#3.2.2.1-Boosting-del-dataset-Pima)\n",
    "            * [3.2.2.2 Boosting del dataset Wisconsin](#3.2.2.2-Boosting-del-dataset-Wisconsin)\n",
    "        * [3.2.3 Random Forest](#3.2.3-Random-Forest)\n",
    "            * [3.2.3.1 Random Forest del dataset Pima](#3.2.3.1-Random-Forest-del-dataset-Pima)\n",
    "            * [3.2.3.2 Random Forest del dataset Wisconsin](#3.2.3.2-Random-Forest-del-dataset-Wisconsin)\n",
    "        * [3.2.4 Gradient Boosting](#3.2.4-Gradient-Boosting)\n",
    "            * [3.2.4.1 Gradient Boosting del dataset Pima](#3.2.4.1-Gradient-Boosting-del-dataset-Pima)\n",
    "            * [3.2.4.2 Gradient Boosting del dataset Wisconsin](#3.2.4.2-Gradient-Boosting-del-dataset-Wisconsin)\n",
    "    * [3.3 Resultados obtenidos tras usar Bagging, Boosting, Random Forest, Gradient Boosting y nuestro Ensemble](#3.3-Resultados-obtenidos-tras-usar-Bagging,-Boosting,-Random-Forest,-Gradient-Boosting-y-nuestro-Ensemble)\n",
    "* [4. Selección de variables](#4.-Selección-de-variables)\n",
    "    * [4.1 Método filter basado en rankings y la importancia de las variables para evaluar distintos subconjuntos](#4.1-Método-filter-basado-en-rankings-y-la-importancia-de-las-variables-para-evaluar-distintos-subconjuntos)\n",
    "        * [4.1.1 Método filter basado en rankings](#4.1.1-Método-filter-basado-en-rankings)\n",
    "            * [4.1.1.1 Método filter basado en rankings con dataset Pima](#4.1.1.1-Método-filter-basado-en-rankings-con-dataset-Pima)\n",
    "            * [4.1.1.2 Método filter basado en rankings con dataset Wisconsin](#4.1.1.2-Método-filter-basado-en-rankings-con-dataset-Wisconsin)\n",
    "        * [4.1.2 Método filter basado en la importancia de las variables](#4.1.2-Método-filter-basado-en-la-importancia-de-las-variables)\n",
    "            * [4.1.2.1 Método filter basado en la importancia de las variables con dataset Pima](#4.1.2.1-Método-filter-basado-en-la-importancia-de-las-variables-con-dataset-Pima)\n",
    "            * [4.1.2.2 Método filter basado en la importancia de las variables con dataset Wisconsin](#4.1.2.2-Método-filter-basado-en-la-importancia-de-las-variables-con-dataset-Wisconsin)\n",
    "    * [4.2 Algoritmo de búsqueda recursiva wrapper](#4.2-Algoritmo-de-búsqueda-recursiva-wrapper)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Antes de empezar..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargamos las librerias necesarias para poder realizar la práctica.\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn.metrics as metrics\n",
    "from scipy import stats, integrate\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn import tree\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.utils import resample\n",
    "from sklearn import neighbors\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set(color_codes=True)\n",
    "\n",
    "# Este codigo configura matplotlib para una representacion adecuada\n",
    "%matplotlib inline\n",
    "mpl.rcParams[\"figure.figsize\"] = \"8, 4\"\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")\n",
    "\n",
    "# Este codigo hace una tabla HTML\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "def printTable(list):\n",
    "    table = \"\"\"<table>%s</table>\"\"\"\n",
    "    row = \"\"\"<tr>%s</tr>\"\"\"\n",
    "    cell = \"\"\"<td>%s</td>\"\"\"\n",
    "    report =  table % ''.join([row % (cell % x[0] + cell % x[1]) for x in results])\n",
    "    display(HTML(report))\n",
    "\n",
    "# Establecemos una semilla para poder obtener siempre los resultados\n",
    "import random\n",
    "seed=1234\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Carga de los Datasets\n",
    "Cargamos un Dataset, para esta práctica usaremos 'Pima' y 'Wisconsin'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargamos el fichero con el conjunto de datos. OJO: colocar el fichero de datos en el mismo directorio que esta libreta\n",
    "df = {}\n",
    "dfAttributes = {}\n",
    "dfLabel = {}\n",
    "df[1] = pd.read_csv(\"pima.csv\", dtype={ \"label\": 'category'})\n",
    "\n",
    "# Preprocesamiento de pima manual (Cambiar los 0's por la media)\n",
    "df[1][\"plas\"].replace(0, df[1][\"plas\"].mean(), inplace=True)\n",
    "df[1][\"pres\"].replace(0, df[1][\"pres\"].mean(), inplace=True)\n",
    "df[1][\"skin\"].replace(0, df[1][\"skin\"].mean(), inplace=True)\n",
    "df[1][\"insu\"].replace(0, df[1][\"insu\"].mean(), inplace=True)\n",
    "df[1][\"mass\"].replace(0, df[1][\"mass\"].mean(), inplace=True)\n",
    "\n",
    "dfAttributes[1] = df[1].drop('label', 1)\n",
    "dfLabel[1] = df[1]['label']\n",
    "\n",
    "df[2] = pd.read_csv(\"wisconsin.csv\", dtype={ \"label\": 'category'})\n",
    "\n",
    "# Preprocesamiento de Wisconsin manual (Eliminamos la columna patientId y cambiamos los nan's por la media)\n",
    "df[2].drop('patientId', 1, inplace=True)\n",
    "\n",
    "df[2][\"clumpThickness\"].replace(np.nan, df[2][\"clumpThickness\"].mean(), inplace=True)\n",
    "df[2][\"cellSize\"].replace(np.nan, df[2][\"cellSize\"].mean(), inplace=True)\n",
    "df[2][\"CellShape\"].replace(np.nan, df[2][\"CellShape\"].mean(), inplace=True)\n",
    "df[2][\"marginalAdhesion\"].replace(np.nan, df[2][\"marginalAdhesion\"].mean(), inplace=True)\n",
    "df[2][\"epithelialSize\"].replace(np.nan, df[2][\"epithelialSize\"].mean(), inplace=True)\n",
    "df[2][\"bareNuclei\"].replace(np.nan, df[2][\"bareNuclei\"].mean(), inplace=True)\n",
    "df[2][\"blandChromatin\"].replace(np.nan, df[2][\"blandChromatin\"].mean(), inplace=True)\n",
    "df[2][\"normalNucleoli\"].replace(np.nan, df[2][\"normalNucleoli\"].mean(), inplace=True)\n",
    "df[2][\"mitoses\"].replace(np.nan, df[2][\"mitoses\"].mean(), inplace=True)\n",
    "\n",
    "dfAttributes[2] = df[2].drop('label', 1)\n",
    "dfLabel[2] = df[2]['label']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Dividimos cada Dataset en train/test para realizar los experimentos.\n",
    "# Para ello realizados un Holdout estratificado.\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_atts = {}\n",
    "test_atts = {}\n",
    "train_label = {}\n",
    "test_label = {}\n",
    "# df[1]\n",
    "train_atts[1], test_atts[1], train_label[1], test_label[1] = train_test_split( \n",
    "    dfAttributes[1],\n",
    "    dfLabel[1],\n",
    "    test_size=0.2,\n",
    "    random_state=seed,\n",
    "    stratify=dfLabel[1])\n",
    "\n",
    "# df[2]\n",
    "train_atts[2], test_atts[2], train_label[2], test_label[2] = train_test_split( \n",
    "    dfAttributes[2],\n",
    "    dfLabel[2],\n",
    "    test_size=0.2,\n",
    "    random_state=seed,\n",
    "    stratify=dfLabel[2])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Uso y ajuste de métodos basados en ensembles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Implementación básica de un ensemble de manera manual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensemble básico\n",
    "# Parametros:\n",
    "    # train_atts [Dict]: conjunto de entrenamiento que contiene los atributos (solo se usa para aprender un modelo)\n",
    "    # train_label [Dict]: conjunto de entrenamiento que contiene las soluciones (solo se usa para aprender un modelo)\n",
    "    # test_atts [Dict]: conjunto de test que contiene los atributos (solo se usar para validar)\n",
    "    # test_label [Dict]: conjunto de test que contiene los soluciones (solo se usa para validar)\n",
    "    # n_estimator [Integer]: número de clasificadores a emplear\n",
    "    # reemplazo [True,False]: indica si el muestreo de las instancias se realizará con/sin remplazo\n",
    "    # n_sample [Intenger]: número de instancias a usar para cada clasificador\n",
    "    # random [Intenger]: semilla\n",
    "    # kn [Intenger]: número de vecinos a usar para cada clasificador\n",
    "    # muestrAtts [Float - [0.0,1.0]]: indica si se hace muestreo o no de atributos. 1.0: sin muestreo, <1.0: muestreo\n",
    "    #                                 más o menos agresivo.\n",
    "def ensemble(train_atts, train_label, test_atts, test_label, n_estimator, reemplazo, n_sampl, random, kn, muestrAtts):\n",
    "    # Definimos una lista que contendrá el número de clasificadores a usar definido por el usuario\n",
    "    knn = [None] * n_estimator\n",
    "    # Lista auxiliar, al comienzo se usa para guardar el conjunto de train_label de las instancias muestreadas\n",
    "    lista = [None] * n_sampl\n",
    "    # Lista que contendrá el resultado de la prediccion sobre test\n",
    "    result = [None] * len(test_atts)\n",
    "    # Por cada estimador indicado por el usuario...\n",
    "    for i in range(0,n_estimator):\n",
    "        # Realizamos una copia de test y training (esto solo tiene utilidad cuando hay muestreo de variables)\n",
    "        train_attsCopy = train_atts.copy()\n",
    "        test_attsCopy = test_atts.copy()\n",
    "        # Comprobamos si hay muestreo de variables\n",
    "        if muestrAtts != 1:\n",
    "            # Si la hay...\n",
    "            auxRandom = 0\n",
    "            # Por cada varible predictora...\n",
    "            for x in train_atts.keys():\n",
    "                # Obtenemos un aleatorio entre 0.0 y 0.999999 y comprobamos si eliminamos o no la variable\n",
    "                np.random.seed(random+auxRandom)\n",
    "                if (np.random.random())>=muestrAtts:\n",
    "                    del train_attsCopy[x]\n",
    "                    del test_attsCopy[x]\n",
    "                auxRandom+=1\n",
    "                # Si se da el caso de que se eliminan más de la mitad de las variables, se dejará de seguir eliminando\n",
    "                # con esto evitamos eliminar todas las variables y que el algoritmo arroje un error.\n",
    "                if len(train_attsCopy.keys())<=(len(train_atts.keys())/2):\n",
    "                    break\n",
    "        # Obtenemos una lista de datos muestreada (con o sin remplazo, según indique \"reemplazo\")\n",
    "        listaDatosMuestreados = {}\n",
    "        listaDatosMuestreados = resample(train_attsCopy, n_samples=n_sampl, random_state=random, replace=reemplazo)\n",
    "        # Obtenemos los indices de todas las instancias con el fin de poder localizarlas en train_label.\n",
    "        indices = listaDatosMuestreados.index\n",
    "        for k in range(0,n_sampl):\n",
    "            lista[k]=(train_label.get(indices[k]))\n",
    "        # Aprendemos un modelo con los vecinos indicados por el usuario\n",
    "        model = neighbors.KNeighborsClassifier(kn)\n",
    "        # Lo entrenamos con los datos de train muestreados\n",
    "        knn[i] = model.fit(listaDatosMuestreados, lista)\n",
    "        # Nos guardamos la prediccion realizada sobre el conjunto de test\n",
    "        knn[i]=knn[i].predict(test_attsCopy)\n",
    "        # Cambiamos el random con el fin de que los nuevos estimadores no sean todos iguales\n",
    "        random+=25\n",
    "    # Restablecemos el random impuesto al comienzo de la práctica, con el fin de evitar cambios no deseados\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # Ahora realizamos un agrupamiento de predicciones para cada instancia de test, para ello escogemos la predicci\n",
    "    for j in range(0, len(test_atts)):\n",
    "        # Restablecemos la lista auxiliar y ahora tendrá la función de almacenar todas las predicciones realizadas por\n",
    "        # cada uno de los estimadores sobre una determinada instancia del conjunto de test\n",
    "        lista = [None] * n_estimator\n",
    "        # \"Rescatamos\" la prediccion realizada por cada estimador\n",
    "        for i in range(0,n_estimator):\n",
    "            lista.append(knn[i][j])\n",
    "    \n",
    "        mayor = 0\n",
    "        clase = None\n",
    "        # Una vez obtenida todas las predicciones sobre una instancia de test sobre una lista, procedemos a realizar\n",
    "        # un voto por la mayoria, para ello comprobaremos cada una de las categorias que puede tomar la variable clase\n",
    "        for i in range(0, len(test_label.cat.categories)):\n",
    "            # Realizamos el recuento de número de apariciones de una categoria y lo almacenamos...\n",
    "            aux = lista.count(test_label.cat.categories[i])\n",
    "            # Si es mayor a lo que teníamos almacenado, nos lo guardamos\n",
    "            if aux >= mayor:\n",
    "                mayor = aux\n",
    "                clase = test_label.cat.categories[i]\n",
    "        # Una vez comprobada todas las posibles categorias que puede tomar la variable clase, almacenamos la prediccion\n",
    "        # mayoritaria en result\n",
    "        result[j] = clase\n",
    "    # Por último, retornamos una lista con todas las predicciones realizadas sobre el conjunto de test\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a probar nuestro ensemble con el dataset Pima con los siguiente parámetros:\n",
    "    - train_atts [Dict]: train_atts[1]\n",
    "    - train_label [Dict]: train_label[1]\n",
    "    - test_atts [Dict]: test_atts[1]\n",
    "    - test_label [Dict]: test_label[1]\n",
    "    - n_estimator [Integer]: 100 clasificadores a emplear\n",
    "    - reemplazo [True,False]: True, muestreo de las instancias con remplazo\n",
    "    - n_sample [Intenger]: 150 instancias para cada clasificador\n",
    "    - random [Intenger]: seed, semilla establecida en el sistema\n",
    "    - kn [Intenger]: 5 vecinos para cada clasificador\n",
    "    - muestrAtts [Float - [0.0,1.0]]: 0.5, con muestreo de atributos al 50%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.70779220779220775"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clasif=ensemble(train_atts[1],train_label[1], test_atts[1], test_label[1], 100,True,150,seed,5,0.5)\n",
    "# Validamos las predicciones obtenidas a partir de test por nuestro ensemble con respecto a su categoria correcta,\n",
    "# obteniendo así la bondad de nuestro ensemble ante nuevos casos.\n",
    "metrics.accuracy_score(test_label[1],clasif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora, procedemos a hacer lo mismo con Wisconsin, cambiando los parámetros de train_atts, train_label, test_atts y test_label con los correspondientes a Wisconsin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.94999999999999996"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clasif=ensemble(train_atts[2],train_label[2], test_atts[2], test_label[2], 100,True,150,seed,5,0.5)\n",
    "metrics.accuracy_score(test_label[2],clasif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Uso de Bagging, Boosting, Random Forest y Gradient Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2.1 Bagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funcion que itera los diferentes parámetros que puede tomar un ensemble bagging, para ello itera los parámetros\n",
    "# pasados por parámetros, desde lo mínimo hasta lo indicado por el usuario\n",
    "def baggingMax(train_atts, train_label, cv, scoring,  estimator, n_estimators, max_samples, random_state):\n",
    "    # Nos almacenamos el Score y la configuración que ha obtenido el mejor Score para mostrarlo en pantalla\n",
    "    maxScore = 0.0\n",
    "    config = (None, None, None)\n",
    "    for i in range(1,n_estimators+1):\n",
    "        j=0.1\n",
    "        while j<=max_samples:\n",
    "            for k in [True,False]:\n",
    "                # Se ejecuta el ensemble...\n",
    "                bagg = BaggingClassifier(estimator, \n",
    "                          n_estimators = i,\n",
    "                          max_samples = j,\n",
    "                          bootstrap = k,\n",
    "                          random_state=random_state)\n",
    "                # Y se hace una validación cruzada con train\n",
    "                scores_bagg = cross_val_score(bagg, train_atts, train_label, cv=cv, scoring=scoring)\n",
    "                # Si el score ha sido mejor que el que teníamos, lo almacenamos y lo mostramos en pantalla como\n",
    "                # posible candidato\n",
    "                if scores_bagg.mean() > maxScore:\n",
    "                    maxScore = scores_bagg.mean()\n",
    "                    config = (i,j,k)\n",
    "                    print(\"Configuracion candidata: \")\n",
    "                    print(scoring, \": %0.2f (+/- %0.2f)\" % (scores_bagg.mean(), scores_bagg.std()*2))\n",
    "                    print(\"Estimator: \" , estimator)\n",
    "                    print(\"N_estimators: \" , config[0])\n",
    "                    print(\"Max_Samples: \" , config[1])\n",
    "                    print(\"Bootstrap: \" , config[2])\n",
    "                    print(\"Random_state: \" , random_state)\n",
    "                    \n",
    "            j+=0.1\n",
    "    # Configuración que ha obtenido el mejor score\n",
    "    print(\"---------------- RESULTADO --------------\")\n",
    "    print(\"Configuracion mas optima: \")\n",
    "    print(scoring , \": \" , maxScore)\n",
    "    print(\"Estimator: \" , estimator)\n",
    "    print(\"N_estimators: \" , config[0])\n",
    "    print(\"Max_Samples: \" , config[1])\n",
    "    print(\"Bootstrap: \" , config[2])\n",
    "    print(\"Random_state: \" , random_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Nota__: dado que todos los algoritmos creados para obtener la mejor configuración son similares (solo cambian los parámetros a iterar y el ensemble a ejecutar), solo se comentará este algoritmo, los demás se dan por explicados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.1.1 Bagging del dataset Pima"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuracion candidata: \n",
      "accuracy : 0.60 (+/- 0.10)\n",
      "Estimator:  DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best')\n",
      "N_estimators:  1\n",
      "Max_Samples:  0.1\n",
      "Bootstrap:  True\n",
      "Random_state:  1234\n",
      "Configuracion candidata: \n",
      "accuracy : 0.69 (+/- 0.05)\n",
      "Estimator:  DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best')\n",
      "N_estimators:  1\n",
      "Max_Samples:  0.1\n",
      "Bootstrap:  False\n",
      "Random_state:  1234\n",
      "Configuracion candidata: \n",
      "accuracy : 0.71 (+/- 0.07)\n",
      "Estimator:  DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best')\n",
      "N_estimators:  1\n",
      "Max_Samples:  0.2\n",
      "Bootstrap:  False\n",
      "Random_state:  1234\n",
      "Configuracion candidata: \n",
      "accuracy : 0.73 (+/- 0.08)\n",
      "Estimator:  DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best')\n",
      "N_estimators:  1\n",
      "Max_Samples:  0.30000000000000004\n",
      "Bootstrap:  False\n",
      "Random_state:  1234\n",
      "Configuracion candidata: \n",
      "accuracy : 0.73 (+/- 0.02)\n",
      "Estimator:  DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best')\n",
      "N_estimators:  1\n",
      "Max_Samples:  0.6\n",
      "Bootstrap:  False\n",
      "Random_state:  1234\n",
      "Configuracion candidata: \n",
      "accuracy : 0.73 (+/- 0.07)\n",
      "Estimator:  DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best')\n",
      "N_estimators:  1\n",
      "Max_Samples:  0.7\n",
      "Bootstrap:  False\n",
      "Random_state:  1234\n",
      "Configuracion candidata: \n",
      "accuracy : 0.74 (+/- 0.03)\n",
      "Estimator:  DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best')\n",
      "N_estimators:  2\n",
      "Max_Samples:  0.6\n",
      "Bootstrap:  True\n",
      "Random_state:  1234\n",
      "Configuracion candidata: \n",
      "accuracy : 0.74 (+/- 0.03)\n",
      "Estimator:  DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best')\n",
      "N_estimators:  2\n",
      "Max_Samples:  0.6\n",
      "Bootstrap:  False\n",
      "Random_state:  1234\n",
      "Configuracion candidata: \n",
      "accuracy : 0.75 (+/- 0.09)\n",
      "Estimator:  DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best')\n",
      "N_estimators:  3\n",
      "Max_Samples:  0.30000000000000004\n",
      "Bootstrap:  False\n",
      "Random_state:  1234\n",
      "Configuracion candidata: \n",
      "accuracy : 0.76 (+/- 0.02)\n",
      "Estimator:  DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best')\n",
      "N_estimators:  3\n",
      "Max_Samples:  0.6\n",
      "Bootstrap:  True\n",
      "Random_state:  1234\n",
      "Configuracion candidata: \n",
      "accuracy : 0.76 (+/- 0.04)\n",
      "Estimator:  DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best')\n",
      "N_estimators:  3\n",
      "Max_Samples:  0.6\n",
      "Bootstrap:  False\n",
      "Random_state:  1234\n",
      "Configuracion candidata: \n",
      "accuracy : 0.76 (+/- 0.05)\n",
      "Estimator:  DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best')\n",
      "N_estimators:  3\n",
      "Max_Samples:  0.7999999999999999\n",
      "Bootstrap:  False\n",
      "Random_state:  1234\n",
      "Configuracion candidata: \n",
      "accuracy : 0.77 (+/- 0.05)\n",
      "Estimator:  DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best')\n",
      "N_estimators:  3\n",
      "Max_Samples:  0.8999999999999999\n",
      "Bootstrap:  True\n",
      "Random_state:  1234\n",
      "Configuracion candidata: \n",
      "accuracy : 0.77 (+/- 0.08)\n",
      "Estimator:  DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best')\n",
      "N_estimators:  4\n",
      "Max_Samples:  0.30000000000000004\n",
      "Bootstrap:  False\n",
      "Random_state:  1234\n",
      "Configuracion candidata: \n",
      "accuracy : 0.78 (+/- 0.06)\n",
      "Estimator:  DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best')\n",
      "N_estimators:  5\n",
      "Max_Samples:  0.7\n",
      "Bootstrap:  False\n",
      "Random_state:  1234\n",
      "Configuracion candidata: \n",
      "accuracy : 0.78 (+/- 0.04)\n",
      "Estimator:  DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best')\n",
      "N_estimators:  8\n",
      "Max_Samples:  0.8999999999999999\n",
      "Bootstrap:  True\n",
      "Random_state:  1234\n",
      "Configuracion candidata: \n",
      "accuracy : 0.79 (+/- 0.05)\n",
      "Estimator:  DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best')\n",
      "N_estimators:  13\n",
      "Max_Samples:  0.2\n",
      "Bootstrap:  True\n",
      "Random_state:  1234\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuracion candidata: \n",
      "accuracy : 0.79 (+/- 0.04)\n",
      "Estimator:  DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best')\n",
      "N_estimators:  21\n",
      "Max_Samples:  0.2\n",
      "Bootstrap:  True\n",
      "Random_state:  1234\n",
      "Configuracion candidata: \n",
      "accuracy : 0.79 (+/- 0.04)\n",
      "Estimator:  DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best')\n",
      "N_estimators:  22\n",
      "Max_Samples:  0.2\n",
      "Bootstrap:  True\n",
      "Random_state:  1234\n",
      "Configuracion candidata: \n",
      "accuracy : 0.79 (+/- 0.05)\n",
      "Estimator:  DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best')\n",
      "N_estimators:  23\n",
      "Max_Samples:  0.2\n",
      "Bootstrap:  True\n",
      "Random_state:  1234\n",
      "Configuracion candidata: \n",
      "accuracy : 0.80 (+/- 0.05)\n",
      "Estimator:  DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best')\n",
      "N_estimators:  24\n",
      "Max_Samples:  0.2\n",
      "Bootstrap:  True\n",
      "Random_state:  1234\n",
      "---------------- RESULTADO --------------\n",
      "Configuracion mas optima: \n",
      "accuracy :  0.798147090551\n",
      "Estimator:  DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best')\n",
      "N_estimators:  24\n",
      "Max_Samples:  0.2\n",
      "Bootstrap:  True\n",
      "Random_state:  1234\n"
     ]
    }
   ],
   "source": [
    "baggingMax(train_atts[1], train_label[1], 3, \"accuracy\", tree.DecisionTreeClassifier(), 60, 1, seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cogemos los parámetros que han maximizado el score del ensemble y lo ejecutamos a parte con el fin de obtener su Accuracy con respecto al conjunto de test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.69480519480519476"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bagg = BaggingClassifier(tree.DecisionTreeClassifier(), \n",
    "                          n_estimators = 24,\n",
    "                          max_samples = 0.2,\n",
    "                          bootstrap = True,\n",
    "                          random_state=seed)\n",
    "\n",
    "\n",
    "bagg.fit(train_atts[1], train_label[1])\n",
    "prediction = bagg.predict(test_atts[1])\n",
    "metrics.accuracy_score(test_label[1], prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como podemos observar la configuración óptima obtenida ha sido la siguiente:\n",
    "\n",
    "Configuracion más óptima: \n",
    "- Accuracy :  0.798147090551\n",
    "- Estimator:  DecisionTreeClassifier(\n",
    "                        class_weight=None, criterion='gini', max_depth=None,\n",
    "                        max_features=None, max_leaf_nodes=None,\n",
    "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "                        min_samples_leaf=1, min_samples_split=2,\n",
    "                        min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
    "                        splitter='best')\n",
    "- N_estimators:  24\n",
    "- Max_Samples:  0.2\n",
    "- Bootstrap:  True\n",
    "- Random_state:  1234\n",
    "- Accuracy tras ejecutar test: 0.69480519480519476\n",
    "\n",
    "El hecho de aumentar de forma muy alta el número de modelos (n_estimators) ha sido para comprobar si se producía alguna mejora, sin embargo nos ha llevado a un gasto de tiempo innecesario dado que la mejor configuración se encuentra en 24 estimadores o modelos.\n",
    "\n",
    "Podemos observar también las configuraciones candidatas que se van evaluando hasta obtener la más óptima, y de ello obtenemos las siguientes conclusiones:\n",
    "- Ante un número determinado de estimadores, el tamaño de la muestra juega un papel fundamental en el aumento del accuracy, dado que cuánto mayor sea el conjunto de entrenamiento mejor será nuestro algoritmo.\n",
    "- Sin embargo, cuando no sea posible mejorar a través del crecimiento de la muestra, será necesario incrementar el número de modelos/estimadores utilizados para conseguir una mejor generalización del conjunto de datos.\n",
    "\n",
    "A menor tamaño de la muestra a utilizar, menor importancia del bootstrap debido a que la probabilidad de que se repitan es baja, sin embargo a mayor tamaño, mayor penalización de la misma dado que se pierde conocimiento, debido a que aumenta la probabilidad de que se repitan las instancias de la muestra.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.74025974025974028"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clasif=ensemble(train_atts[1],train_label[1], test_atts[1], test_label[1], 60, False,150,seed,20,1)\n",
    "metrics.accuracy_score(test_label[1],clasif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En cuanto a nuestro ensemble, obtenemos un accuracy más alto que el anterior. Esto es debido a que KNN como tal no crea un modelo, sino que predice conforme a los datos de train. Por lo tanto, comparar un KNN con un árbol de decisión no debería ser lo más correcto."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.1.2 Bagging del dataset Wisconsin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuracion candidata: \n",
      "accuracy : 0.92 (+/- 0.05)\n",
      "Estimator:  DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best')\n",
      "N_estimators:  1\n",
      "Max_Samples:  0.1\n",
      "Bootstrap:  True\n",
      "Random_state:  1234\n",
      "Configuracion candidata: \n",
      "accuracy : 0.93 (+/- 0.04)\n",
      "Estimator:  DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best')\n",
      "N_estimators:  1\n",
      "Max_Samples:  0.1\n",
      "Bootstrap:  False\n",
      "Random_state:  1234\n",
      "Configuracion candidata: \n",
      "accuracy : 0.94 (+/- 0.05)\n",
      "Estimator:  DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best')\n",
      "N_estimators:  1\n",
      "Max_Samples:  0.5\n",
      "Bootstrap:  False\n",
      "Random_state:  1234\n",
      "Configuracion candidata: \n",
      "accuracy : 0.94 (+/- 0.01)\n",
      "Estimator:  DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best')\n",
      "N_estimators:  1\n",
      "Max_Samples:  0.7999999999999999\n",
      "Bootstrap:  True\n",
      "Random_state:  1234\n",
      "Configuracion candidata: \n",
      "accuracy : 0.95 (+/- 0.02)\n",
      "Estimator:  DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best')\n",
      "N_estimators:  1\n",
      "Max_Samples:  0.9999999999999999\n",
      "Bootstrap:  False\n",
      "Random_state:  1234\n",
      "Configuracion candidata: \n",
      "accuracy : 0.96 (+/- 0.05)\n",
      "Estimator:  DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best')\n",
      "N_estimators:  3\n",
      "Max_Samples:  0.2\n",
      "Bootstrap:  True\n",
      "Random_state:  1234\n",
      "Configuracion candidata: \n",
      "accuracy : 0.96 (+/- 0.04)\n",
      "Estimator:  DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best')\n",
      "N_estimators:  5\n",
      "Max_Samples:  0.30000000000000004\n",
      "Bootstrap:  True\n",
      "Random_state:  1234\n",
      "Configuracion candidata: \n",
      "accuracy : 0.96 (+/- 0.04)\n",
      "Estimator:  DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best')\n",
      "N_estimators:  5\n",
      "Max_Samples:  0.4\n",
      "Bootstrap:  True\n",
      "Random_state:  1234\n",
      "Configuracion candidata: \n",
      "accuracy : 0.96 (+/- 0.02)\n",
      "Estimator:  DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best')\n",
      "N_estimators:  5\n",
      "Max_Samples:  0.8999999999999999\n",
      "Bootstrap:  True\n",
      "Random_state:  1234\n",
      "Configuracion candidata: \n",
      "accuracy : 0.96 (+/- 0.03)\n",
      "Estimator:  DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best')\n",
      "N_estimators:  6\n",
      "Max_Samples:  0.5\n",
      "Bootstrap:  False\n",
      "Random_state:  1234\n",
      "Configuracion candidata: \n",
      "accuracy : 0.97 (+/- 0.03)\n",
      "Estimator:  DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best')\n",
      "N_estimators:  7\n",
      "Max_Samples:  0.30000000000000004\n",
      "Bootstrap:  False\n",
      "Random_state:  1234\n",
      "Configuracion candidata: \n",
      "accuracy : 0.97 (+/- 0.03)\n",
      "Estimator:  DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best')\n",
      "N_estimators:  7\n",
      "Max_Samples:  0.5\n",
      "Bootstrap:  False\n",
      "Random_state:  1234\n",
      "Configuracion candidata: \n",
      "accuracy : 0.97 (+/- 0.04)\n",
      "Estimator:  DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best')\n",
      "N_estimators:  11\n",
      "Max_Samples:  0.6\n",
      "Bootstrap:  True\n",
      "Random_state:  1234\n",
      "---------------- RESULTADO --------------\n",
      "Configuracion mas optima: \n",
      "accuracy :  0.969610718188\n",
      "Estimator:  DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best')\n",
      "N_estimators:  11\n",
      "Max_Samples:  0.6\n",
      "Bootstrap:  True\n",
      "Random_state:  1234\n"
     ]
    }
   ],
   "source": [
    "baggingMax(train_atts[2], train_label[2], 3, \"accuracy\", tree.DecisionTreeClassifier(), 60, 1, seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cogemos los parámetros que han maximizado el score del ensemble y lo ejecutamos a parte con el fin de obtener su Accuracy con respecto al conjunto de test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.95714285714285718"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bagg = BaggingClassifier(tree.DecisionTreeClassifier(), \n",
    "                          n_estimators = 11,\n",
    "                          max_samples = 0.6,\n",
    "                          bootstrap = True,\n",
    "                          random_state=seed)\n",
    "\n",
    "\n",
    "bagg.fit(train_atts[2], train_label[2])\n",
    "prediction = bagg.predict(test_atts[2])\n",
    "metrics.accuracy_score(test_label[2], prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como podemos observar la configuración óptima obtenida ha sido la siguiente:\n",
    "\n",
    "Configuracion más óptima: \n",
    "- Accuracy :  0.969610718188\n",
    "- Estimator:  DecisionTreeClassifier(\n",
    "                        class_weight=None, criterion='gini', max_depth=None,\n",
    "                        max_features=None, max_leaf_nodes=None,\n",
    "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "                        min_samples_leaf=1, min_samples_split=2,\n",
    "                        min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
    "                        splitter='best')\n",
    "- N_estimators:  11\n",
    "- Max_Samples:  0.6\n",
    "- Bootstrap:  True\n",
    "- Random_state:  1234\n",
    "- Accuracy tras ejecutar test: 0.95714285714285718\n",
    "\n",
    "Como podemos observar, en ambos casos (Pima y wisconsin), no es necesario usar demasiados estimadores, sino que la mejora se suele estancar en cuánto a 11 aproximadamente, en el caso de Pima fue necesario más estimadores, por que habría más margen de mejora de Accuracy, en el caso de Wisconsin, con un Accuracy de 0.9696 la mejora es altamente complicada de conseguir.\n",
    "La explicación del proceso de conseguir la configuración óptima es similar al anterior.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.92142857142857137"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clasif=ensemble(train_atts[2],train_label[2], test_atts[2], test_label[2], 60, False,150,seed,20,1)\n",
    "metrics.accuracy_score(test_label[2],clasif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En cuanto a nuestro ensemble, obtenemos un accuracy un poco más bajo que el anterior, además de que no podemos comparar la eficacia de un árbol con un knn. Cómo dijimos anteriormente KNN como tal no crea un modelo, sino que predice conforme a los datos de train. Por lo tanto, comparar un KNN con un árbol de decisión no debería ser lo más correcto."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2.2 Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def boostingMax(train_atts, train_label, cv, scoring,  estimator, n_estimators, learningRate, random_state):\n",
    "    maxScore = 0.0\n",
    "    config = (None, None, None)\n",
    "    for i in range(1,n_estimators+1):\n",
    "        j=0.1\n",
    "        while j<=learningRate:\n",
    "            \n",
    "            boost = AdaBoostClassifier(base_estimator=estimator, \n",
    "                      n_estimators = i,\n",
    "                      learning_rate = j,\n",
    "                      random_state=random_state)\n",
    "            scores_boost = cross_val_score(boost, train_atts, train_label, cv=cv, scoring=scoring)\n",
    "            if scores_boost.mean() > maxScore:\n",
    "                maxScore = scores_boost.mean()\n",
    "                config = (i,j)\n",
    "                print(\"Configuracion candidata: \")\n",
    "                print(scoring, \": %0.2f (+/- %0.2f)\" % (scores_boost.mean(), scores_boost.std()*2))\n",
    "                print(\"Estimator: \" , estimator)\n",
    "                print(\"N_estimators: \" , config[0])\n",
    "                print(\"Learning_rate: \" , config[1])\n",
    "                print(\"Random_state: \" , random_state)\n",
    "\n",
    "            j+=0.1\n",
    "    print(\"---------------- RESULTADO --------------\")\n",
    "    print(\"Configuracion mas optima: \")\n",
    "    print(scoring , \": \" , maxScore)\n",
    "    print(\"Estimator: \" , estimator)\n",
    "    print(\"N_estimators: \" , config[0])\n",
    "    print(\"Learning_rate: \" , config[1])\n",
    "    print(\"Random_state: \" , random_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.2.1 Boosting del dataset Pima"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuracion candidata: \n",
      "accuracy : 0.74 (+/- 0.04)\n",
      "Estimator:  DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best')\n",
      "N_estimators:  1\n",
      "Learning_rate:  0.1\n",
      "Random_state:  1234\n",
      "---------------- RESULTADO --------------\n",
      "Configuracion mas optima: \n",
      "accuracy :  0.740973412019\n",
      "Estimator:  DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best')\n",
      "N_estimators:  1\n",
      "Learning_rate:  0.1\n",
      "Random_state:  1234\n"
     ]
    }
   ],
   "source": [
    "boostingMax(train_atts[1], train_label[1], 3, \"accuracy\", tree.DecisionTreeClassifier(), 100, 1, seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La configuración óptima ha sido la siguiente:\n",
    "\n",
    "Configuracion más óptima: \n",
    "- Accuracy :  0.740973412019\n",
    "- Estimator:  DecisionTreeClassifier(\n",
    "                        class_weight=None, criterion='gini', max_depth=None,\n",
    "                        max_features=None, max_leaf_nodes=None,\n",
    "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "                        min_samples_leaf=1, min_samples_split=2,\n",
    "                        min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
    "                        splitter='best')\n",
    "- N_estimators:  1\n",
    "- Learning_rate:  0.1\n",
    "- Random_state:  1234\n",
    "\n",
    "Como podemos observar, usando un solo estimador, ha sido más que suficiente para no obtener mejora, nosotros creemos que esto es debido a que como todos los casos de training han sido clasificados correctamente, no ha sido necesario usar más estimadores. \n",
    "El boosting básicamente consiste en asignar pesos a cada ejemplo del conjunto de entrenamiento, se aprende un modelo y para el siguiente clasificador a usar hará más incapié en aquellos casos que no han sido bien clasificados modificando los pesos. \n",
    "En nuestro caso, dado que nuestro árbol de decisión no tiene ningún límite de expansión, todos los casos fueron clasificados, sobreajustado a training, por lo tanto esto lo que ha hecho ha sido ejecutar un decisionTree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuracion candidata: \n",
      "accuracy : 0.76 (+/- 0.01)\n",
      "Estimator:  DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=5,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best')\n",
      "N_estimators:  1\n",
      "Learning_rate:  0.1\n",
      "Random_state:  1234\n",
      "Configuracion candidata: \n",
      "accuracy : 0.76 (+/- 0.00)\n",
      "Estimator:  DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=5,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best')\n",
      "N_estimators:  2\n",
      "Learning_rate:  0.1\n",
      "Random_state:  1234\n",
      "Configuracion candidata: \n",
      "accuracy : 0.76 (+/- 0.01)\n",
      "Estimator:  DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=5,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best')\n",
      "N_estimators:  2\n",
      "Learning_rate:  0.2\n",
      "Random_state:  1234\n",
      "Configuracion candidata: \n",
      "accuracy : 0.76 (+/- 0.04)\n",
      "Estimator:  DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=5,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best')\n",
      "N_estimators:  2\n",
      "Learning_rate:  0.6\n",
      "Random_state:  1234\n",
      "Configuracion candidata: \n",
      "accuracy : 0.77 (+/- 0.01)\n",
      "Estimator:  DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=5,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best')\n",
      "N_estimators:  2\n",
      "Learning_rate:  0.7\n",
      "Random_state:  1234\n",
      "Configuracion candidata: \n",
      "accuracy : 0.77 (+/- 0.00)\n",
      "Estimator:  DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=5,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best')\n",
      "N_estimators:  2\n",
      "Learning_rate:  0.9999999999999999\n",
      "Random_state:  1234\n",
      "Configuracion candidata: \n",
      "accuracy : 0.77 (+/- 0.03)\n",
      "Estimator:  DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=5,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best')\n",
      "N_estimators:  4\n",
      "Learning_rate:  0.1\n",
      "Random_state:  1234\n",
      "Configuracion candidata: \n",
      "accuracy : 0.77 (+/- 0.03)\n",
      "Estimator:  DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=5,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best')\n",
      "N_estimators:  5\n",
      "Learning_rate:  0.1\n",
      "Random_state:  1234\n",
      "Configuracion candidata: \n",
      "accuracy : 0.77 (+/- 0.06)\n",
      "Estimator:  DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=5,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best')\n",
      "N_estimators:  7\n",
      "Learning_rate:  0.9999999999999999\n",
      "Random_state:  1234\n",
      "Configuracion candidata: \n",
      "accuracy : 0.78 (+/- 0.03)\n",
      "Estimator:  DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=5,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best')\n",
      "N_estimators:  12\n",
      "Learning_rate:  0.7999999999999999\n",
      "Random_state:  1234\n",
      "Configuracion candidata: \n",
      "accuracy : 0.78 (+/- 0.03)\n",
      "Estimator:  DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=5,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best')\n",
      "N_estimators:  25\n",
      "Learning_rate:  0.2\n",
      "Random_state:  1234\n",
      "Configuracion candidata: \n",
      "accuracy : 0.78 (+/- 0.03)\n",
      "Estimator:  DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=5,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best')\n",
      "N_estimators:  29\n",
      "Learning_rate:  0.7999999999999999\n",
      "Random_state:  1234\n",
      "Configuracion candidata: \n",
      "accuracy : 0.79 (+/- 0.02)\n",
      "Estimator:  DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=5,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best')\n",
      "N_estimators:  32\n",
      "Learning_rate:  0.2\n",
      "Random_state:  1234\n",
      "Configuracion candidata: \n",
      "accuracy : 0.79 (+/- 0.03)\n",
      "Estimator:  DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=5,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best')\n",
      "N_estimators:  35\n",
      "Learning_rate:  0.2\n",
      "Random_state:  1234\n",
      "Configuracion candidata: \n",
      "accuracy : 0.79 (+/- 0.03)\n",
      "Estimator:  DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=5,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best')\n",
      "N_estimators:  42\n",
      "Learning_rate:  0.2\n",
      "Random_state:  1234\n",
      "Configuracion candidata: \n",
      "accuracy : 0.79 (+/- 0.03)\n",
      "Estimator:  DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=5,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best')\n",
      "N_estimators:  43\n",
      "Learning_rate:  0.2\n",
      "Random_state:  1234\n",
      "Configuracion candidata: \n",
      "accuracy : 0.79 (+/- 0.04)\n",
      "Estimator:  DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=5,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best')\n",
      "N_estimators:  97\n",
      "Learning_rate:  0.8999999999999999\n",
      "Random_state:  1234\n",
      "Configuracion candidata: \n",
      "accuracy : 0.79 (+/- 0.05)\n",
      "Estimator:  DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=5,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best')\n",
      "N_estimators:  98\n",
      "Learning_rate:  0.8999999999999999\n",
      "Random_state:  1234\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------- RESULTADO --------------\n",
      "Configuracion mas optima: \n",
      "accuracy :  0.794831524843\n",
      "Estimator:  DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=5,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best')\n",
      "N_estimators:  98\n",
      "Learning_rate:  0.8999999999999999\n",
      "Random_state:  1234\n"
     ]
    }
   ],
   "source": [
    "boostingMax(train_atts[1], train_label[1], 3, \"accuracy\", tree.DecisionTreeClassifier(max_depth=5), 100, 1, seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cogemos los parámetros que han maximizado el score del ensemble y lo ejecutamos a parte con el fin de obtener su Accuracy con respecto al conjunto de test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6428571428571429"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boost = AdaBoostClassifier(base_estimator=tree.DecisionTreeClassifier(max_depth=5), \n",
    "                      n_estimators = 98,\n",
    "                      learning_rate = 0.89999,\n",
    "                      random_state=seed)\n",
    "\n",
    "\n",
    "boost.fit(train_atts[1], train_label[1])\n",
    "prediction = boost.predict(test_atts[1])\n",
    "metrics.accuracy_score(test_label[1], prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este caso obtenemos la siguiente configuración óptima:\n",
    "\n",
    "Configuracion más óptima: \n",
    "- Accuracy :  0.794831524843\n",
    "- Estimator:  DecisionTreeClassifier(\n",
    "                        class_weight=None, criterion='gini', max_depth=5,\n",
    "                        max_features=None, max_leaf_nodes=None,\n",
    "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "                        min_samples_leaf=1, min_samples_split=2,\n",
    "                        min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
    "                        splitter='best')\n",
    "- N_estimators:  98\n",
    "- Learning_rate:  0.8999999999999999\n",
    "- Random_state:  1234\n",
    "- Accuracy tras ejecutar test: 0.6428571428571429\n",
    "\n",
    "Este ejemplo ejecutado anteriormente apoya nuestra afirmación.\n",
    "Como podemos observar al limitar el árbol de decisión, si que tendrá utilidad usar el boosting y vemos que obtenemos un mejor accuracy.\n",
    "Este caso generaliza mucho mejor que el anterior dado que está limitado, y además podemos ver que learning_rate de los modelos anteriores tiene una buena importancia. Hay muchas veces en las que el learning_rate es bajo, esto es debido a que a veces es necesario partir prácticamente de un modelo nuevo con el fin de seguir clasificando nuevas instancias y no quedarnos estancados en los modelos anteriores que ya no aportan mejores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.74675324675324672"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clasif=ensemble(train_atts[1],train_label[1], test_atts[1], test_label[1], 100, False,150,seed,20,1)\n",
    "metrics.accuracy_score(test_label[1],clasif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En cuanto a nuestro ensemble, obtenemos un accuracy más alto que el anterior. Esto es debido a que KNN como tal no crea un modelo, sino que predice conforme a los datos de train. Por lo tanto, comparar un KNN con un árbol de decisión no debería ser lo más correcto."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.2.2 Boosting del dataset Wisconsin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuracion candidata: \n",
      "accuracy : 0.96 (+/- 0.01)\n",
      "Estimator:  DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best')\n",
      "N_estimators:  1\n",
      "Learning_rate:  0.1\n",
      "Random_state:  1234\n",
      "---------------- RESULTADO --------------\n",
      "Configuracion mas optima: \n",
      "accuracy :  0.955264217124\n",
      "Estimator:  DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best')\n",
      "N_estimators:  1\n",
      "Learning_rate:  0.1\n",
      "Random_state:  1234\n"
     ]
    }
   ],
   "source": [
    "boostingMax(train_atts[2], train_label[2], 3, \"accuracy\", tree.DecisionTreeClassifier(), 100, 1, seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuracion candidata: \n",
      "accuracy : 0.95 (+/- 0.02)\n",
      "Estimator:  DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=5,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best')\n",
      "N_estimators:  1\n",
      "Learning_rate:  0.1\n",
      "Random_state:  1234\n",
      "Configuracion candidata: \n",
      "accuracy : 0.95 (+/- 0.02)\n",
      "Estimator:  DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=5,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best')\n",
      "N_estimators:  2\n",
      "Learning_rate:  0.1\n",
      "Random_state:  1234\n",
      "Configuracion candidata: \n",
      "accuracy : 0.96 (+/- 0.01)\n",
      "Estimator:  DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=5,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best')\n",
      "N_estimators:  2\n",
      "Learning_rate:  0.2\n",
      "Random_state:  1234\n",
      "Configuracion candidata: \n",
      "accuracy : 0.96 (+/- 0.01)\n",
      "Estimator:  DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=5,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best')\n",
      "N_estimators:  3\n",
      "Learning_rate:  0.4\n",
      "Random_state:  1234\n",
      "Configuracion candidata: \n",
      "accuracy : 0.96 (+/- 0.04)\n",
      "Estimator:  DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=5,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best')\n",
      "N_estimators:  4\n",
      "Learning_rate:  0.30000000000000004\n",
      "Random_state:  1234\n",
      "Configuracion candidata: \n",
      "accuracy : 0.96 (+/- 0.01)\n",
      "Estimator:  DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=5,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best')\n",
      "N_estimators:  5\n",
      "Learning_rate:  0.4\n",
      "Random_state:  1234\n",
      "Configuracion candidata: \n",
      "accuracy : 0.97 (+/- 0.03)\n",
      "Estimator:  DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=5,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best')\n",
      "N_estimators:  6\n",
      "Learning_rate:  0.2\n",
      "Random_state:  1234\n",
      "Configuracion candidata: \n",
      "accuracy : 0.97 (+/- 0.02)\n",
      "Estimator:  DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=5,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best')\n",
      "N_estimators:  8\n",
      "Learning_rate:  0.4\n",
      "Random_state:  1234\n",
      "Configuracion candidata: \n",
      "accuracy : 0.97 (+/- 0.02)\n",
      "Estimator:  DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=5,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best')\n",
      "N_estimators:  9\n",
      "Learning_rate:  0.4\n",
      "Random_state:  1234\n",
      "Configuracion candidata: \n",
      "accuracy : 0.97 (+/- 0.03)\n",
      "Estimator:  DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=5,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best')\n",
      "N_estimators:  11\n",
      "Learning_rate:  0.8999999999999999\n",
      "Random_state:  1234\n",
      "Configuracion candidata: \n",
      "accuracy : 0.97 (+/- 0.02)\n",
      "Estimator:  DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=5,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best')\n",
      "N_estimators:  12\n",
      "Learning_rate:  0.9999999999999999\n",
      "Random_state:  1234\n",
      "Configuracion candidata: \n",
      "accuracy : 0.98 (+/- 0.01)\n",
      "Estimator:  DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=5,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best')\n",
      "N_estimators:  24\n",
      "Learning_rate:  0.9999999999999999\n",
      "Random_state:  1234\n",
      "Configuracion candidata: \n",
      "accuracy : 0.98 (+/- 0.02)\n",
      "Estimator:  DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=5,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best')\n",
      "N_estimators:  37\n",
      "Learning_rate:  0.9999999999999999\n",
      "Random_state:  1234\n",
      "---------------- RESULTADO --------------\n",
      "Configuracion mas optima: \n",
      "accuracy :  0.976750426466\n",
      "Estimator:  DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=5,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best')\n",
      "N_estimators:  37\n",
      "Learning_rate:  0.9999999999999999\n",
      "Random_state:  1234\n"
     ]
    }
   ],
   "source": [
    "boostingMax(train_atts[2], train_label[2], 3, \"accuracy\", tree.DecisionTreeClassifier(max_depth=5), 100, 1, seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cogemos los parámetros que han maximizado el score del ensemble y lo ejecutamos a parte con el fin de obtener su Accuracy con respecto al conjunto de test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.94999999999999996"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boost = AdaBoostClassifier(base_estimator=tree.DecisionTreeClassifier(max_depth=5), \n",
    "                      n_estimators = 37,\n",
    "                      learning_rate = 0.99999,\n",
    "                      random_state=seed)\n",
    "\n",
    "\n",
    "boost.fit(train_atts[2], train_label[2])\n",
    "prediction = boost.predict(test_atts[2])\n",
    "metrics.accuracy_score(test_label[2], prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtenemos la siguiente configuración óptima:\n",
    "\n",
    "Configuracion más óptima: \n",
    "- Accuracy :  0.976750426466\n",
    "- Estimator:  DecisionTreeClassifier(\n",
    "                        class_weight=None, criterion='gini', max_depth=5,\n",
    "                        max_features=None, max_leaf_nodes=None,\n",
    "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "                        min_samples_leaf=1, min_samples_split=2,\n",
    "                        min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
    "                        splitter='best')\n",
    "- N_estimators:  37\n",
    "- Learning_rate:  0.9999999999999999\n",
    "- Random_state:  1234\n",
    "- Accuracy tras ejecutar test: 0.94999999999999996\n",
    "\n",
    "Como podemos observar al limitar el árbol de decisión, si que tendrá utilidad usar el boosting y vemos que obtenemos un mejor accuracy.\n",
    "Una vez que obtenemos un modelo que clasifica muy bien nuevos casos y el accuracy ya es muy alto, interesa más mantener un learning_rate alto, dado que el modelo ya es \"perfecto\" como tal y la mejora no es significativa.\n",
    "Por lo demás, la explicación es similar a la anterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.92142857142857137"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clasif=ensemble(train_atts[2],train_label[2], test_atts[2], test_label[2], 100, False,150,seed,20,1)\n",
    "metrics.accuracy_score(test_label[2],clasif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En cuanto a nuestro ensemble, obtenemos un accuracy un poco más bajo que el anterior, además de que no podemos comparar la eficacia de un árbol con un knn. Cómo dijimos anteriormente KNN como tal no crea un modelo, sino que predice conforme a los datos de train. Por lo tanto, comparar un KNN con un árbol de decisión no debería ser lo más correcto."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2.3 Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def randomForestMax(train_atts, train_label, cv, scoring, n_estimators, max_depth, min_samples_split, min_samples_leaf, random_state):\n",
    "    maxScore = 0.0\n",
    "    config = (None, None, None, None, None, None)\n",
    "    for i in range(1,n_estimators+1):\n",
    "        for j in [\"gini\",\"entropy\"]:\n",
    "            for k in [None] + [a for a in range(1,max_depth+1)]:\n",
    "                for l in range(2,min_samples_split+1):\n",
    "                    for m in range(1,min_samples_leaf+1):\n",
    "                        for n in [\"True\",\"False\"]:\n",
    "                            rf = RandomForestClassifier(n_estimators = i, \n",
    "                                                        criterion=j, \n",
    "                                                        max_depth=k,\n",
    "                                                        min_samples_split = l,\n",
    "                                                        min_samples_leaf = m,\n",
    "                                                        bootstrap = n,\n",
    "                                                        random_state = random_state)\n",
    "                            scores_rf = cross_val_score(rf, train_atts, train_label, cv=cv, scoring=scoring)\n",
    "                            if scores_rf.mean() > maxScore:\n",
    "                                maxScore = scores_rf.mean()\n",
    "                                config = (i,j,k,l,m,n)\n",
    "                                print(\"Configuracion candidata: \")\n",
    "                                print(scoring, \": %0.2f (+/- %0.2f)\" % (scores_rf.mean(), scores_rf.std()*2))\n",
    "                                print(\"Criterion: \" , config[1])\n",
    "                                print(\"N_estimators: \" , config[0])\n",
    "                                print(\"Max_depth: \" , config[2])\n",
    "                                print(\"Min_samples_split: \" , config[3])\n",
    "                                print(\"Min_samples_leaf: \" , config[4])\n",
    "                                print(\"Bootstrap: \" , config[5])\n",
    "                                print(\"Random_state: \" , random_state)\n",
    "\n",
    "    print(\"---------------- RESULTADO --------------\")\n",
    "    print(\"Configuracion mas optima: \")\n",
    "    print(scoring , \": \" , maxScore)\n",
    "    print(\"Criterion: \" , config[1])\n",
    "    print(\"N_estimators: \" , config[0])\n",
    "    print(\"Max_depth: \" , config[2])\n",
    "    print(\"Min_samples_split: \" , config[3])\n",
    "    print(\"Min_samples_leaf: \" , config[4])\n",
    "    print(\"Bootstrap: \" , config[5])\n",
    "    print(\"Random_state: \" , random_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.3.1 Random Forest del dataset Pima"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuracion candidata: \n",
      "accuracy : 0.69 (+/- 0.04)\n",
      "Criterion:  gini\n",
      "N_estimators:  1\n",
      "Max_depth:  None\n",
      "Min_samples_split:  2\n",
      "Min_samples_leaf:  1\n",
      "Bootstrap:  True\n",
      "Random_state:  1234\n",
      "Configuracion candidata: \n",
      "accuracy : 0.74 (+/- 0.07)\n",
      "Criterion:  gini\n",
      "N_estimators:  1\n",
      "Max_depth:  None\n",
      "Min_samples_split:  2\n",
      "Min_samples_leaf:  2\n",
      "Bootstrap:  True\n",
      "Random_state:  1234\n",
      "Configuracion candidata: \n",
      "accuracy : 0.74 (+/- 0.05)\n",
      "Criterion:  gini\n",
      "N_estimators:  2\n",
      "Max_depth:  None\n",
      "Min_samples_split:  2\n",
      "Min_samples_leaf:  2\n",
      "Bootstrap:  True\n",
      "Random_state:  1234\n",
      "Configuracion candidata: \n",
      "accuracy : 0.75 (+/- 0.06)\n",
      "Criterion:  gini\n",
      "N_estimators:  2\n",
      "Max_depth:  None\n",
      "Min_samples_split:  2\n",
      "Min_samples_leaf:  3\n",
      "Bootstrap:  True\n",
      "Random_state:  1234\n",
      "Configuracion candidata: \n",
      "accuracy : 0.76 (+/- 0.02)\n",
      "Criterion:  gini\n",
      "N_estimators:  2\n",
      "Max_depth:  4\n",
      "Min_samples_split:  2\n",
      "Min_samples_leaf:  1\n",
      "Bootstrap:  True\n",
      "Random_state:  1234\n",
      "Configuracion candidata: \n",
      "accuracy : 0.77 (+/- 0.01)\n",
      "Criterion:  gini\n",
      "N_estimators:  3\n",
      "Max_depth:  5\n",
      "Min_samples_split:  2\n",
      "Min_samples_leaf:  3\n",
      "Bootstrap:  True\n",
      "Random_state:  1234\n",
      "Configuracion candidata: \n",
      "accuracy : 0.77 (+/- 0.02)\n",
      "Criterion:  gini\n",
      "N_estimators:  4\n",
      "Max_depth:  5\n",
      "Min_samples_split:  2\n",
      "Min_samples_leaf:  2\n",
      "Bootstrap:  True\n",
      "Random_state:  1234\n",
      "Configuracion candidata: \n",
      "accuracy : 0.77 (+/- 0.03)\n",
      "Criterion:  entropy\n",
      "N_estimators:  4\n",
      "Max_depth:  5\n",
      "Min_samples_split:  2\n",
      "Min_samples_leaf:  3\n",
      "Bootstrap:  True\n",
      "Random_state:  1234\n",
      "Configuracion candidata: \n",
      "accuracy : 0.77 (+/- 0.06)\n",
      "Criterion:  gini\n",
      "N_estimators:  5\n",
      "Max_depth:  5\n",
      "Min_samples_split:  2\n",
      "Min_samples_leaf:  2\n",
      "Bootstrap:  True\n",
      "Random_state:  1234\n",
      "Configuracion candidata: \n",
      "accuracy : 0.79 (+/- 0.02)\n",
      "Criterion:  gini\n",
      "N_estimators:  5\n",
      "Max_depth:  5\n",
      "Min_samples_split:  2\n",
      "Min_samples_leaf:  3\n",
      "Bootstrap:  True\n",
      "Random_state:  1234\n",
      "Configuracion candidata: \n",
      "accuracy : 0.79 (+/- 0.04)\n",
      "Criterion:  gini\n",
      "N_estimators:  6\n",
      "Max_depth:  5\n",
      "Min_samples_split:  2\n",
      "Min_samples_leaf:  3\n",
      "Bootstrap:  True\n",
      "Random_state:  1234\n",
      "Configuracion candidata: \n",
      "accuracy : 0.79 (+/- 0.03)\n",
      "Criterion:  gini\n",
      "N_estimators:  12\n",
      "Max_depth:  5\n",
      "Min_samples_split:  2\n",
      "Min_samples_leaf:  3\n",
      "Bootstrap:  True\n",
      "Random_state:  1234\n",
      "Configuracion candidata: \n",
      "accuracy : 0.80 (+/- 0.06)\n",
      "Criterion:  gini\n",
      "N_estimators:  18\n",
      "Max_depth:  5\n",
      "Min_samples_split:  2\n",
      "Min_samples_leaf:  3\n",
      "Bootstrap:  True\n",
      "Random_state:  1234\n",
      "---------------- RESULTADO --------------\n",
      "Configuracion mas optima: \n",
      "accuracy :  0.798147090551\n",
      "Criterion:  gini\n",
      "N_estimators:  18\n",
      "Max_depth:  5\n",
      "Min_samples_split:  2\n",
      "Min_samples_leaf:  3\n",
      "Bootstrap:  True\n",
      "Random_state:  1234\n"
     ]
    }
   ],
   "source": [
    "randomForestMax(train_atts[1], train_label[1], 3, \"accuracy\", 50, 5, 4, 3, seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cogemos los parámetros que han maximizado el score del ensemble y lo ejecutamos a parte con el fin de obtener su Accuracy con respecto al conjunto de test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.70779220779220775"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf = RandomForestClassifier(n_estimators = 18, \n",
    "                                criterion=\"gini\", \n",
    "                                max_depth=5,\n",
    "                                min_samples_split = 2,\n",
    "                                min_samples_leaf = 3,\n",
    "                                bootstrap = True,\n",
    "                                random_state = seed)\n",
    "\n",
    "\n",
    "rf.fit(train_atts[1], train_label[1])\n",
    "prediction = rf.predict(test_atts[1])\n",
    "metrics.accuracy_score(test_label[1], prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La configuración óptima es la siguiente:\n",
    "\n",
    "Configuracion más óptima: \n",
    "- Accuracy :  0.798147090551\n",
    "- Criterion:  gini\n",
    "- N_estimators:  18\n",
    "- Max_depth:  5\n",
    "- Min_samples_split:  2\n",
    "- Min_samples_leaf:  3\n",
    "- Bootstrap:  True\n",
    "- Random_state:  1234\n",
    "- Accuracy tras ejecutar test: 0.70779220779220775\n",
    "\n",
    "Podemos observar las siguientes características del proceso de mejora de la configuración:\n",
    "- El mejor accuracy se ha obtenido cuando la máxima profundidad ha sido None, esto es debido a que el árbol se ha podido expandir mejor y ha podido clasificar más número de casos de manera correcta y por lo tanto ha obtenido un accuracy mayor en test. Debemos de decir que los modelos son modelos grandes (muy expandidos) y poco prácticos, y preferimos modelos más sencillos (navaja de ockham).\n",
    "- Luego también cuanto menor sea el criterio de división (min_samples_split), se suele obtener mejor accuracy debido a que el árbol se ramifica más y clasifica casos más concretos.\n",
    "- En cuánto al min_samples_leaf cuanto mayor sea, más compacto será el árbol generado y generalizará mejor puesto que limitará la capacidad de expansión del nodo hoja.\n",
    "- El hecho de combinar min_samples_split con min_samples_leaf nos da una alta capacidad de limitación/expansión del árbol, podríamos decir que es una especie de \"prepoda\".\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.74675324675324672"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clasif=ensemble(train_atts[1],train_label[1], test_atts[1], test_label[1], 50, False,150,seed,20,1)\n",
    "metrics.accuracy_score(test_label[1],clasif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En cuanto a nuestro ensemble, obtenemos un accuracy más alto que el anterior. Esto es debido a que KNN como tal no crea un modelo, sino que predice conforme a los datos de train. Por lo tanto, comparar un KNN con un árbol de decisión no debería ser lo más correcto."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.3.2 Random Forest del dataset Wisconsin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuracion candidata: \n",
      "accuracy : 0.95 (+/- 0.01)\n",
      "Criterion:  gini\n",
      "N_estimators:  1\n",
      "Max_depth:  None\n",
      "Min_samples_split:  2\n",
      "Min_samples_leaf:  1\n",
      "Bootstrap:  True\n",
      "Random_state:  1234\n",
      "Configuracion candidata: \n",
      "accuracy : 0.96 (+/- 0.03)\n",
      "Criterion:  gini\n",
      "N_estimators:  1\n",
      "Max_depth:  3\n",
      "Min_samples_split:  2\n",
      "Min_samples_leaf:  1\n",
      "Bootstrap:  True\n",
      "Random_state:  1234\n",
      "Configuracion candidata: \n",
      "accuracy : 0.96 (+/- 0.02)\n",
      "Criterion:  entropy\n",
      "N_estimators:  1\n",
      "Max_depth:  4\n",
      "Min_samples_split:  2\n",
      "Min_samples_leaf:  1\n",
      "Bootstrap:  True\n",
      "Random_state:  1234\n",
      "Configuracion candidata: \n",
      "accuracy : 0.96 (+/- 0.02)\n",
      "Criterion:  entropy\n",
      "N_estimators:  1\n",
      "Max_depth:  4\n",
      "Min_samples_split:  2\n",
      "Min_samples_leaf:  2\n",
      "Bootstrap:  True\n",
      "Random_state:  1234\n",
      "Configuracion candidata: \n",
      "accuracy : 0.96 (+/- 0.03)\n",
      "Criterion:  gini\n",
      "N_estimators:  2\n",
      "Max_depth:  None\n",
      "Min_samples_split:  2\n",
      "Min_samples_leaf:  2\n",
      "Bootstrap:  True\n",
      "Random_state:  1234\n",
      "Configuracion candidata: \n",
      "accuracy : 0.96 (+/- 0.03)\n",
      "Criterion:  entropy\n",
      "N_estimators:  2\n",
      "Max_depth:  2\n",
      "Min_samples_split:  2\n",
      "Min_samples_leaf:  1\n",
      "Bootstrap:  True\n",
      "Random_state:  1234\n",
      "Configuracion candidata: \n",
      "accuracy : 0.96 (+/- 0.02)\n",
      "Criterion:  entropy\n",
      "N_estimators:  3\n",
      "Max_depth:  3\n",
      "Min_samples_split:  2\n",
      "Min_samples_leaf:  2\n",
      "Bootstrap:  True\n",
      "Random_state:  1234\n",
      "Configuracion candidata: \n",
      "accuracy : 0.97 (+/- 0.03)\n",
      "Criterion:  entropy\n",
      "N_estimators:  3\n",
      "Max_depth:  4\n",
      "Min_samples_split:  2\n",
      "Min_samples_leaf:  2\n",
      "Bootstrap:  True\n",
      "Random_state:  1234\n",
      "Configuracion candidata: \n",
      "accuracy : 0.97 (+/- 0.03)\n",
      "Criterion:  entropy\n",
      "N_estimators:  4\n",
      "Max_depth:  5\n",
      "Min_samples_split:  2\n",
      "Min_samples_leaf:  2\n",
      "Bootstrap:  True\n",
      "Random_state:  1234\n",
      "Configuracion candidata: \n",
      "accuracy : 0.97 (+/- 0.04)\n",
      "Criterion:  entropy\n",
      "N_estimators:  5\n",
      "Max_depth:  2\n",
      "Min_samples_split:  2\n",
      "Min_samples_leaf:  1\n",
      "Bootstrap:  True\n",
      "Random_state:  1234\n",
      "Configuracion candidata: \n",
      "accuracy : 0.97 (+/- 0.03)\n",
      "Criterion:  gini\n",
      "N_estimators:  8\n",
      "Max_depth:  None\n",
      "Min_samples_split:  2\n",
      "Min_samples_leaf:  2\n",
      "Bootstrap:  True\n",
      "Random_state:  1234\n",
      "Configuracion candidata: \n",
      "accuracy : 0.97 (+/- 0.04)\n",
      "Criterion:  entropy\n",
      "N_estimators:  8\n",
      "Max_depth:  None\n",
      "Min_samples_split:  2\n",
      "Min_samples_leaf:  2\n",
      "Bootstrap:  True\n",
      "Random_state:  1234\n",
      "Configuracion candidata: \n",
      "accuracy : 0.97 (+/- 0.03)\n",
      "Criterion:  entropy\n",
      "N_estimators:  9\n",
      "Max_depth:  None\n",
      "Min_samples_split:  2\n",
      "Min_samples_leaf:  2\n",
      "Bootstrap:  True\n",
      "Random_state:  1234\n",
      "Configuracion candidata: \n",
      "accuracy : 0.97 (+/- 0.03)\n",
      "Criterion:  gini\n",
      "N_estimators:  13\n",
      "Max_depth:  None\n",
      "Min_samples_split:  3\n",
      "Min_samples_leaf:  1\n",
      "Bootstrap:  True\n",
      "Random_state:  1234\n",
      "---------------- RESULTADO --------------\n",
      "Configuracion mas optima: \n",
      "accuracy :  0.973175780576\n",
      "Criterion:  gini\n",
      "N_estimators:  13\n",
      "Max_depth:  None\n",
      "Min_samples_split:  3\n",
      "Min_samples_leaf:  1\n",
      "Bootstrap:  True\n",
      "Random_state:  1234\n"
     ]
    }
   ],
   "source": [
    "randomForestMax(train_atts[2], train_label[2], 3, \"accuracy\", 50, 5, 4, 2, seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cogemos los parámetros que han maximizado el score del ensemble y lo ejecutamos a parte con el fin de obtener su Accuracy con respecto al conjunto de test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.93571428571428572"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf = RandomForestClassifier(n_estimators = 13, \n",
    "                                criterion=\"gini\", \n",
    "                                max_depth=None,\n",
    "                                min_samples_split = 3,\n",
    "                                min_samples_leaf = 1,\n",
    "                                bootstrap = True,\n",
    "                                random_state = seed)\n",
    "\n",
    "\n",
    "rf.fit(train_atts[2], train_label[2])\n",
    "prediction = rf.predict(test_atts[2])\n",
    "metrics.accuracy_score(test_label[2], prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtenemos la siguiente configuración óptima:\n",
    "\n",
    "Configuracion más óptima: \n",
    "- Accuracy :  0.973175780576\n",
    "- Criterion:  gini\n",
    "- N_estimators:  13\n",
    "- Max_depth:  None\n",
    "- Min_samples_split:  3\n",
    "- Min_samples_leaf:  1\n",
    "- Bootstrap:  True\n",
    "- Random_state:  1234\n",
    "- Accuracy tras ejecutar test: 0.93571428571428572\n",
    "\n",
    "En el caso de wisconsin, podemos ver que la acción elegida es aquella configuración de máxima expansión (max_depth=None, min_samples_split=3, min_samples_leaf=1).\n",
    "También hemos comprobado que para mejorar el Accuracy de la configuración se elige entre aumentar el número de estimadores que se usa, o aumentar la máxima profundidad posible del árbol."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.92142857142857137"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clasif=ensemble(train_atts[2],train_label[2], test_atts[2], test_label[2], 100, False,150,seed,20,1)\n",
    "metrics.accuracy_score(test_label[2],clasif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En cuanto a nuestro ensemble, obtenemos un accuracy un poco más bajo que el anterior, además de que no podemos comparar la eficacia de un árbol con un knn. Cómo dijimos anteriormente KNN como tal no crea un modelo, sino que predice conforme a los datos de train. Por lo tanto, comparar un KNN con un árbol de decisión no debería ser lo más correcto."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2.4 Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradientBoostingMax(train_atts, train_label, cv, scoring, n_estimators, learningRate, subsample, max_depth, min_samples_split, min_samples_leaf, random_state):\n",
    "    maxScore = 0.0\n",
    "    config = (None, None, None, None, None, None)\n",
    "    for i in range(1,n_estimators+1):\n",
    "        j=0.1\n",
    "        while j<=learningRate:\n",
    "            for k in [None] + [a for a in range(1,max_depth+1)]:\n",
    "                for l in range(2,min_samples_split+1):\n",
    "                    for m in range(1,min_samples_leaf+1):\n",
    "                        n=0.1\n",
    "                        while n<=subsample:\n",
    "                            gboost = GradientBoostingClassifier(n_estimators = i, \n",
    "                                                        learning_rate=j, \n",
    "                                                        max_depth=k,\n",
    "                                                        min_samples_split = l,\n",
    "                                                        min_samples_leaf = m,\n",
    "                                                        subsample = n,\n",
    "                                                        random_state = random_state)\n",
    "                            scores_gboost = cross_val_score(gboost, train_atts, train_label, cv=cv, scoring=scoring)\n",
    "                            if scores_gboost.mean() > maxScore:\n",
    "                                maxScore = scores_gboost.mean()\n",
    "                                config = (i,j,k,l,m,n)\n",
    "                                print(\"Configuracion candidata: \")\n",
    "                                print(scoring, \": %0.2f (+/- %0.2f)\" % (scores_gboost.mean(), scores_gboost.std()*2))\n",
    "                                print(\"Learning_rate: \" , config[1])\n",
    "                                print(\"N_estimators: \" , config[0])\n",
    "                                print(\"Max_depth: \" , config[2])\n",
    "                                print(\"Min_samples_split: \" , config[3])\n",
    "                                print(\"Min_samples_leaf: \" , config[4])\n",
    "                                print(\"Subsample: \" , config[5])\n",
    "                                print(\"Random_state: \" , random_state)\n",
    "                            n+=0.3\n",
    "            j+=0.3\n",
    "\n",
    "    print(\"---------------- RESULTADO --------------\")\n",
    "    print(\"Configuracion mas optima: \")\n",
    "    print(scoring , \": \" , maxScore)\n",
    "    print(\"Learning_rate: \" , config[1])\n",
    "    print(\"N_estimators: \" , config[0])\n",
    "    print(\"Max_depth: \" , config[2])\n",
    "    print(\"Min_samples_split: \" , config[3])\n",
    "    print(\"Min_samples_leaf: \" , config[4])\n",
    "    print(\"Subsample: \" , config[5])\n",
    "    print(\"Random_state: \" , random_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.4.1 Gradient Boosting del dataset Pima"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuracion candidata: \n",
      "accuracy : 0.65 (+/- 0.00)\n",
      "Learning_rate:  0.1\n",
      "N_estimators:  1\n",
      "Max_depth:  None\n",
      "Min_samples_split:  2\n",
      "Min_samples_leaf:  1\n",
      "Subsample:  0.1\n",
      "Random_state:  1234\n",
      "Configuracion candidata: \n",
      "accuracy : 0.69 (+/- 0.04)\n",
      "Learning_rate:  0.4\n",
      "N_estimators:  1\n",
      "Max_depth:  None\n",
      "Min_samples_split:  2\n",
      "Min_samples_leaf:  1\n",
      "Subsample:  0.1\n",
      "Random_state:  1234\n",
      "Configuracion candidata: \n",
      "accuracy : 0.73 (+/- 0.03)\n",
      "Learning_rate:  0.4\n",
      "N_estimators:  1\n",
      "Max_depth:  None\n",
      "Min_samples_split:  2\n",
      "Min_samples_leaf:  1\n",
      "Subsample:  0.4\n",
      "Random_state:  1234\n",
      "Configuracion candidata: \n",
      "accuracy : 0.74 (+/- 0.03)\n",
      "Learning_rate:  0.4\n",
      "N_estimators:  1\n",
      "Max_depth:  None\n",
      "Min_samples_split:  2\n",
      "Min_samples_leaf:  1\n",
      "Subsample:  1.0\n",
      "Random_state:  1234\n",
      "Configuracion candidata: \n",
      "accuracy : 0.76 (+/- 0.04)\n",
      "Learning_rate:  0.4\n",
      "N_estimators:  1\n",
      "Max_depth:  None\n",
      "Min_samples_split:  2\n",
      "Min_samples_leaf:  2\n",
      "Subsample:  1.0\n",
      "Random_state:  1234\n",
      "Configuracion candidata: \n",
      "accuracy : 0.76 (+/- 0.01)\n",
      "Learning_rate:  0.7\n",
      "N_estimators:  1\n",
      "Max_depth:  5\n",
      "Min_samples_split:  2\n",
      "Min_samples_leaf:  2\n",
      "Subsample:  1.0\n",
      "Random_state:  1234\n",
      "Configuracion candidata: \n",
      "accuracy : 0.77 (+/- 0.09)\n",
      "Learning_rate:  0.4\n",
      "N_estimators:  2\n",
      "Max_depth:  3\n",
      "Min_samples_split:  2\n",
      "Min_samples_leaf:  2\n",
      "Subsample:  0.4\n",
      "Random_state:  1234\n",
      "Configuracion candidata: \n",
      "accuracy : 0.77 (+/- 0.07)\n",
      "Learning_rate:  0.4\n",
      "N_estimators:  2\n",
      "Max_depth:  3\n",
      "Min_samples_split:  3\n",
      "Min_samples_leaf:  1\n",
      "Subsample:  0.4\n",
      "Random_state:  1234\n",
      "Configuracion candidata: \n",
      "accuracy : 0.77 (+/- 0.07)\n",
      "Learning_rate:  0.4\n",
      "N_estimators:  2\n",
      "Max_depth:  5\n",
      "Min_samples_split:  2\n",
      "Min_samples_leaf:  2\n",
      "Subsample:  1.0\n",
      "Random_state:  1234\n",
      "Configuracion candidata: \n",
      "accuracy : 0.78 (+/- 0.04)\n",
      "Learning_rate:  0.7\n",
      "N_estimators:  2\n",
      "Max_depth:  3\n",
      "Min_samples_split:  2\n",
      "Min_samples_leaf:  1\n",
      "Subsample:  0.7\n",
      "Random_state:  1234\n",
      "Configuracion candidata: \n",
      "accuracy : 0.78 (+/- 0.05)\n",
      "Learning_rate:  0.4\n",
      "N_estimators:  3\n",
      "Max_depth:  5\n",
      "Min_samples_split:  2\n",
      "Min_samples_leaf:  2\n",
      "Subsample:  1.0\n",
      "Random_state:  1234\n",
      "Configuracion candidata: \n",
      "accuracy : 0.78 (+/- 0.07)\n",
      "Learning_rate:  0.7\n",
      "N_estimators:  4\n",
      "Max_depth:  5\n",
      "Min_samples_split:  4\n",
      "Min_samples_leaf:  1\n",
      "Subsample:  1.0\n",
      "Random_state:  1234\n",
      "Configuracion candidata: \n",
      "accuracy : 0.79 (+/- 0.06)\n",
      "Learning_rate:  0.4\n",
      "N_estimators:  8\n",
      "Max_depth:  2\n",
      "Min_samples_split:  2\n",
      "Min_samples_leaf:  2\n",
      "Subsample:  0.4\n",
      "Random_state:  1234\n",
      "Configuracion candidata: \n",
      "accuracy : 0.79 (+/- 0.05)\n",
      "Learning_rate:  0.1\n",
      "N_estimators:  9\n",
      "Max_depth:  5\n",
      "Min_samples_split:  3\n",
      "Min_samples_leaf:  1\n",
      "Subsample:  0.4\n",
      "Random_state:  1234\n",
      "Configuracion candidata: \n",
      "accuracy : 0.79 (+/- 0.02)\n",
      "Learning_rate:  0.7\n",
      "N_estimators:  13\n",
      "Max_depth:  5\n",
      "Min_samples_split:  2\n",
      "Min_samples_leaf:  2\n",
      "Subsample:  1.0\n",
      "Random_state:  1234\n",
      "Configuracion candidata: \n",
      "accuracy : 0.79 (+/- 0.03)\n",
      "Learning_rate:  0.4\n",
      "N_estimators:  16\n",
      "Max_depth:  None\n",
      "Min_samples_split:  2\n",
      "Min_samples_leaf:  2\n",
      "Subsample:  1.0\n",
      "Random_state:  1234\n",
      "---------------- RESULTADO --------------\n",
      "Configuracion mas optima: \n",
      "accuracy :  0.794815660892\n",
      "Learning_rate:  0.4\n",
      "N_estimators:  16\n",
      "Max_depth:  None\n",
      "Min_samples_split:  2\n",
      "Min_samples_leaf:  2\n",
      "Subsample:  1.0\n",
      "Random_state:  1234\n"
     ]
    }
   ],
   "source": [
    "gradientBoostingMax(train_atts[1], train_label[1], 3, \"accuracy\", 25, 1, 1, 5, 4, 2, seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cogemos los parámetros que han maximizado el score del ensemble y lo ejecutamos a parte con el fin de obtener su Accuracy con respecto al conjunto de test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.68181818181818177"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gBoost = GradientBoostingClassifier(n_estimators = 16, \n",
    "                                    learning_rate=0.4, \n",
    "                                    max_depth=None,\n",
    "                                    min_samples_split = 2,\n",
    "                                    min_samples_leaf = 2,\n",
    "                                    subsample = 1.0,\n",
    "                                    random_state = seed)\n",
    "\n",
    "\n",
    "gBoost.fit(train_atts[1], train_label[1])\n",
    "prediction = gBoost.predict(test_atts[1])\n",
    "metrics.accuracy_score(test_label[1], prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtenemos la siguiente configuración óptima:\n",
    "\n",
    "Configuracion más óptima: \n",
    "- Accuracy :  0.794815660892\n",
    "- Learning_rate:  0.4\n",
    "- N_estimators:  16\n",
    "- Max_depth:  None\n",
    "- Min_samples_split:  2\n",
    "- Min_samples_leaf:  2\n",
    "- Subsample:  1.0\n",
    "- Random_state:  1234\n",
    "- Accuracy tras ejecutar test: 0.68181818181818177\n",
    "\n",
    "Con respecto a los parámetros similares al de randomForest la explicación es similar al randomForest de Wisconsin, dado que generamos un árbol sin limite de expansión (o con un límite muy bajo: max_depth: None, min_samples_split: 2 y min_samples_leaf: 2).\n",
    "\n",
    "En cuanto al learning_rate el promedio es 0.5 puesto que nos interesa obtener estimadores que no estén basados al 100% en en el anterior, sino que queremos mejorar.\n",
    "\n",
    "Y en cuanto al subsample, cuanto menor es, más reduce la varianza e incrementa el sesgo. En este caso, se prefiere un mayor subsample debido a que preferimos unos datos más variados (que generalicen mejor) y con menor sesgo (que estén más cerca de la predicción esperada)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.74675324675324672"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clasif=ensemble(train_atts[1],train_label[1], test_atts[1], test_label[1], 100, False,150,seed,20,1)\n",
    "metrics.accuracy_score(test_label[1],clasif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En cuanto a nuestro ensemble, obtenemos un accuracy más alto que el anterior. Esto es debido a que KNN como tal no crea un modelo, sino que predice conforme a los datos de train. Por lo tanto, comparar un KNN con un árbol de decisión no debería ser lo más correcto."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.4.2 Gradient Boosting del dataset Wisconsin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuracion candidata: \n",
      "accuracy : 0.65 (+/- 0.00)\n",
      "Learning_rate:  0.1\n",
      "N_estimators:  1\n",
      "Max_depth:  None\n",
      "Min_samples_split:  2\n",
      "Min_samples_leaf:  1\n",
      "Subsample:  0.1\n",
      "Random_state:  1234\n",
      "Configuracion candidata: \n",
      "accuracy : 0.93 (+/- 0.03)\n",
      "Learning_rate:  0.4\n",
      "N_estimators:  1\n",
      "Max_depth:  None\n",
      "Min_samples_split:  2\n",
      "Min_samples_leaf:  1\n",
      "Subsample:  0.1\n",
      "Random_state:  1234\n",
      "Configuracion candidata: \n",
      "accuracy : 0.94 (+/- 0.02)\n",
      "Learning_rate:  0.4\n",
      "N_estimators:  1\n",
      "Max_depth:  None\n",
      "Min_samples_split:  2\n",
      "Min_samples_leaf:  1\n",
      "Subsample:  0.4\n",
      "Random_state:  1234\n",
      "Configuracion candidata: \n",
      "accuracy : 0.95 (+/- 0.03)\n",
      "Learning_rate:  0.4\n",
      "N_estimators:  1\n",
      "Max_depth:  None\n",
      "Min_samples_split:  2\n",
      "Min_samples_leaf:  1\n",
      "Subsample:  1.0\n",
      "Random_state:  1234\n",
      "Configuracion candidata: \n",
      "accuracy : 0.95 (+/- 0.01)\n",
      "Learning_rate:  0.4\n",
      "N_estimators:  1\n",
      "Max_depth:  5\n",
      "Min_samples_split:  2\n",
      "Min_samples_leaf:  1\n",
      "Subsample:  1.0\n",
      "Random_state:  1234\n",
      "Configuracion candidata: \n",
      "accuracy : 0.96 (+/- 0.01)\n",
      "Learning_rate:  0.7\n",
      "N_estimators:  1\n",
      "Max_depth:  4\n",
      "Min_samples_split:  2\n",
      "Min_samples_leaf:  2\n",
      "Subsample:  1.0\n",
      "Random_state:  1234\n",
      "Configuracion candidata: \n",
      "accuracy : 0.96 (+/- 0.01)\n",
      "Learning_rate:  1.0\n",
      "N_estimators:  1\n",
      "Max_depth:  None\n",
      "Min_samples_split:  2\n",
      "Min_samples_leaf:  2\n",
      "Subsample:  1.0\n",
      "Random_state:  1234\n",
      "Configuracion candidata: \n",
      "accuracy : 0.96 (+/- 0.01)\n",
      "Learning_rate:  1.0\n",
      "N_estimators:  1\n",
      "Max_depth:  4\n",
      "Min_samples_split:  2\n",
      "Min_samples_leaf:  2\n",
      "Subsample:  1.0\n",
      "Random_state:  1234\n",
      "Configuracion candidata: \n",
      "accuracy : 0.96 (+/- 0.02)\n",
      "Learning_rate:  1.0\n",
      "N_estimators:  1\n",
      "Max_depth:  5\n",
      "Min_samples_split:  2\n",
      "Min_samples_leaf:  2\n",
      "Subsample:  1.0\n",
      "Random_state:  1234\n",
      "Configuracion candidata: \n",
      "accuracy : 0.96 (+/- 0.03)\n",
      "Learning_rate:  1.0\n",
      "N_estimators:  3\n",
      "Max_depth:  2\n",
      "Min_samples_split:  2\n",
      "Min_samples_leaf:  1\n",
      "Subsample:  0.7\n",
      "Random_state:  1234\n",
      "Configuracion candidata: \n",
      "accuracy : 0.96 (+/- 0.02)\n",
      "Learning_rate:  0.7\n",
      "N_estimators:  4\n",
      "Max_depth:  None\n",
      "Min_samples_split:  2\n",
      "Min_samples_leaf:  2\n",
      "Subsample:  0.7\n",
      "Random_state:  1234\n",
      "Configuracion candidata: \n",
      "accuracy : 0.96 (+/- 0.02)\n",
      "Learning_rate:  0.4\n",
      "N_estimators:  5\n",
      "Max_depth:  None\n",
      "Min_samples_split:  2\n",
      "Min_samples_leaf:  2\n",
      "Subsample:  0.7\n",
      "Random_state:  1234\n",
      "Configuracion candidata: \n",
      "accuracy : 0.97 (+/- 0.03)\n",
      "Learning_rate:  0.4\n",
      "N_estimators:  6\n",
      "Max_depth:  1\n",
      "Min_samples_split:  2\n",
      "Min_samples_leaf:  1\n",
      "Subsample:  0.1\n",
      "Random_state:  1234\n",
      "Configuracion candidata: \n",
      "accuracy : 0.97 (+/- 0.04)\n",
      "Learning_rate:  0.1\n",
      "N_estimators:  9\n",
      "Max_depth:  5\n",
      "Min_samples_split:  2\n",
      "Min_samples_leaf:  1\n",
      "Subsample:  0.7\n",
      "Random_state:  1234\n",
      "Configuracion candidata: \n",
      "accuracy : 0.97 (+/- 0.03)\n",
      "Learning_rate:  0.4\n",
      "N_estimators:  12\n",
      "Max_depth:  2\n",
      "Min_samples_split:  2\n",
      "Min_samples_leaf:  1\n",
      "Subsample:  0.4\n",
      "Random_state:  1234\n",
      "Configuracion candidata: \n",
      "accuracy : 0.97 (+/- 0.03)\n",
      "Learning_rate:  0.1\n",
      "N_estimators:  13\n",
      "Max_depth:  None\n",
      "Min_samples_split:  2\n",
      "Min_samples_leaf:  2\n",
      "Subsample:  0.7\n",
      "Random_state:  1234\n",
      "Configuracion candidata: \n",
      "accuracy : 0.97 (+/- 0.02)\n",
      "Learning_rate:  0.4\n",
      "N_estimators:  15\n",
      "Max_depth:  None\n",
      "Min_samples_split:  4\n",
      "Min_samples_leaf:  1\n",
      "Subsample:  0.7\n",
      "Random_state:  1234\n",
      "---------------- RESULTADO --------------\n",
      "Configuracion mas optima: \n",
      "accuracy :  0.973175780576\n",
      "Learning_rate:  0.4\n",
      "N_estimators:  15\n",
      "Max_depth:  None\n",
      "Min_samples_split:  4\n",
      "Min_samples_leaf:  1\n",
      "Subsample:  0.7\n",
      "Random_state:  1234\n"
     ]
    }
   ],
   "source": [
    "gradientBoostingMax(train_atts[2], train_label[2], 3, \"accuracy\", 25, 1, 1, 5, 4, 2, seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cogemos los parámetros que han maximizado el score del ensemble y lo ejecutamos a parte con el fin de obtener su Accuracy con respecto al conjunto de test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.94285714285714284"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gBoost = GradientBoostingClassifier(n_estimators = 15, \n",
    "                                    learning_rate=0.4, \n",
    "                                    max_depth=None,\n",
    "                                    min_samples_split = 4,\n",
    "                                    min_samples_leaf = 1,\n",
    "                                    subsample = 0.7,\n",
    "                                    random_state = seed)\n",
    "\n",
    "\n",
    "gBoost.fit(train_atts[2], train_label[2])\n",
    "prediction = gBoost.predict(test_atts[2])\n",
    "metrics.accuracy_score(test_label[2], prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtenemos la siguiente configuración óptima:\n",
    "\n",
    "Configuracion más óptima: \n",
    "- Accuracy :  0.973175780576\n",
    "- Learning_rate:  0.4\n",
    "- N_estimators:  15\n",
    "- Max_depth:  None\n",
    "- Min_samples_split:  4\n",
    "- Min_samples_leaf:  1\n",
    "- Subsample:  0.7\n",
    "- Random_state:  1234\n",
    "- Accuracy tras ejecutar test: 0.94285714285714284\n",
    "\n",
    "Con respecto a los parámetros similares al de randomForest la explicación es similar al randomForest de Wisconsin, dado que generamos un árbol sin limite de expansión (o con un límite muy bajo: max_depth: None, min_samples_split: 4 y min_samples_leaf: 1), aunque en este caso se podría pensar que min_samples_split limita bastante, pero el hecho de tener max_depth = None, esta limitación es escasa, además que si observamos las configuraciones subóptimas a esta, podemos ver que dichos parámetros no han obtenido una gran diferencia en cuanto a la mejora obtenida.\n",
    "\n",
    "En cuanto al learning_rate el promedio es 0.5 puesto que nos interesa obtener estimadores que no estén basados al 100% en en el anterior, sino que queremos mejorar.\n",
    "\n",
    "Y en cuanto al subsample, cuanto menor es, más reduce la varianza e incrementa el sesgo. En este caso, se prefiere un mayor subsample debido a que preferimos unos datos más variados (que generalicen mejor) y con menor sesgo (que estén más cerca de la predicción esperada)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.92142857142857137"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clasif=ensemble(train_atts[2],train_label[2], test_atts[2], test_label[2], 100, False,150,seed,20,1)\n",
    "metrics.accuracy_score(test_label[2],clasif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En cuanto a nuestro ensemble, obtenemos un accuracy un poco más bajo que el anterior, además de que no podemos comparar la eficacia de un árbol con un knn. Cómo dijimos anteriormente KNN como tal no crea un modelo, sino que predice conforme a los datos de train. Por lo tanto, comparar un KNN con un árbol de decisión no debería ser lo más correcto."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Resultados obtenidos tras usar Bagging, Boosting, Random Forest, Gradient Boosting y nuestro Ensemble"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con el dataset Pima, los Accuracies obtenidos tras testear con test han sido los siguientes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><tr><td>Nuestro ensemble</td><td>0.7077922077922078</td></tr><tr><td>Bagging</td><td>0.6948051948051948</td></tr><tr><td>Random Forest</td><td>0.7077922077922078</td></tr><tr><td>Boosting</td><td>0.6428571428571429</td></tr><tr><td>Gradient Boosting</td><td>0.6818181818181818</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "results = ((\"Nuestro ensemble\", 0.70779220779220775), \n",
    "           (\"Bagging\", 0.69480519480519476), \n",
    "           (\"Random Forest\", 0.70779220779220775),\n",
    "           (\"Boosting\", 0.6428571428571429),\n",
    "           (\"Gradient Boosting\", 0.68181818181818177))\n",
    "\n",
    "printTable(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con el dataset Wisconsin, los Accuracies obtenidos tras testear con test han sido los siguiente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><tr><td>Nuestro ensemble</td><td>0.95</td></tr><tr><td>Bagging</td><td>0.9571428571428572</td></tr><tr><td>Random Forest</td><td>0.9357142857142857</td></tr><tr><td>Boosting</td><td>0.95</td></tr><tr><td>Gradient Boosting</td><td>0.9428571428571428</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "results = ((\"Nuestro ensemble\", 0.94999999999999996), \n",
    "           (\"Bagging\", 0.95714285714285718), \n",
    "           (\"Random Forest\", 0.93571428571428572),\n",
    "           (\"Boosting\", 0.94999999999999996),\n",
    "           (\"Gradient Boosting\", 0.94285714285714284))\n",
    "\n",
    "printTable(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como podemos observar, sorprendentemente nuestro ensemble se ha posicionado entre los primeros en obtener una mejor predicción. En cuanto a los ensembles implementados en Scikit, podemos observar una gran diversidad de resultados, siendo altamente complicado establecer un orden general de quién es mejor con respecto a los demás. Esto último, está altamente condicionado con el dataset, siendo en algunos casos unos ensembles más importantes que otros."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Selección de variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Método filter basado en rankings y la importancia de las variables para evaluar distintos subconjuntos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.1 Método filter basado en rankings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función que retorna el ranking de las variables de un determinado dataset\n",
    "def filterRank(metrica, df):\n",
    "    # Obtiene los scores de aplicar una determianda metríca al conjunto de datos\n",
    "    scores = list(metrica)\n",
    "    names = list(df.keys())\n",
    "    # Se obtiene una lista ranks que junta los scores con su respectiva variable predictora\n",
    "    ranks = sorted( list(zip(scores, names)), reverse=True )\n",
    "    return ranks\n",
    "\n",
    "# Función que retorna un par de diccionarios (train_atts y test_atts) eliminando las variables con menor Score,\n",
    "# se indicará por parámetro la métrica a emplear, el número de variables a salvar, el dataset completo [solo usado\n",
    "# para obtener el conjunto de variables predictoras], train y test\n",
    "def selectK(metrica, k, df, train_atts, train_label, test_atts):\n",
    "    # Se obtiene una lista indicando las variables a salvar con respecto a los parámetros dados\n",
    "    # Nota: fss.get_support se obtiene una lista con True's y False's indicando que variables se salvan o no, están\n",
    "    # en orden con respecto a train_atts\n",
    "    fss = SelectKBest(metrica, k=k).fit(train_atts, train_label)\n",
    "    train_attsCopy = train_atts.copy()\n",
    "    test_attsCopy = test_atts.copy()\n",
    "    print(\"Variables seleccionadas: \")\n",
    "    # Bucle que mostrará las variables seleccionadas para salvar y eliminará el resto (esto se hace tanto en train\n",
    "    # como en test, se realiza con test el fin de evitar errores a la hora de predecir con éste)\n",
    "    for i in range(0,len(fss.get_support())):\n",
    "        if (fss.get_support())[i]:\n",
    "            print((df.keys())[i])\n",
    "        else:\n",
    "            del train_attsCopy[(df.keys())[i]]\n",
    "            del test_attsCopy[(df.keys())[i]]\n",
    "    print(train_attsCopy.shape)\n",
    "    return (train_attsCopy, test_attsCopy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1.1.1 Método filter basado en rankings con dataset Pima"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ejecutando el algoritmo de filter basado en rankings, vamos a obtener el ranking de las variables ordenado por su importancia, y posteriormente procederemos a obtener un nuevo diccionario con el que nos quedaremos con solo un número determinado de variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.1288987268794517, 'plas'),\n",
       " (0.094889792970892284, 'mass'),\n",
       " (0.067196685611855322, 'age'),\n",
       " (0.064457008631584101, 'preg'),\n",
       " (0.040198824529407595, 'pres'),\n",
       " (0.035925099264167093, 'insu'),\n",
       " (0.014890438929848759, 'skin'),\n",
       " (0.0, 'pedi')]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrica=mutual_info_classif(train_atts[1], train_label[1], random_state=seed)\n",
    "filterRank(metrica, df[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variables seleccionadas: \n",
      "preg\n",
      "plas\n",
      "mass\n",
      "age\n",
      "(614, 4)\n",
      "     preg   plas       mass  age\n",
      "54      0   74.0  27.800000   22\n",
      "156     8  183.0  23.300000   32\n",
      "38      1  108.0  27.100000   24\n",
      "646     2  101.0  21.800000   22\n",
      "447     4   97.0  28.200000   22\n",
      "759     1  109.0  25.400000   21\n",
      "576     0  101.0  35.700000   26\n",
      "660     4  189.0  28.500000   37\n",
      "431     2   71.0  28.000000   22\n",
      "568     5  155.0  38.700000   34\n",
      "486     8  124.0  28.700000   52\n",
      "388     9  145.0  30.300000   53\n",
      "539     6  102.0  30.800000   36\n",
      "355     5  109.0  36.000000   60\n",
      "51      6  125.0  30.000000   32\n",
      "525     0  139.0  22.100000   21\n",
      "421     0  113.0  31.000000   21\n",
      "557     9  184.0  30.000000   49\n",
      "691    10  108.0  32.400000   42\n",
      "395     6  125.0  27.600000   49\n",
      "61      6  137.0  24.200000   55\n",
      "598     1  130.0  25.900000   22\n",
      "410     1  108.0  35.500000   24\n",
      "327     7  196.0  39.800000   41\n",
      "122     1   90.0  25.100000   25\n",
      "430     2   92.0  31.600000   24\n",
      "417     3  108.0  26.000000   25\n",
      "66     13  106.0  36.600000   45\n",
      "160    11  136.0  28.300000   42\n",
      "590     0  135.0  40.600000   26\n",
      "..    ...    ...        ...  ...\n",
      "86      0  188.0  32.000000   22\n",
      "642     3  122.0  23.000000   40\n",
      "494     1   97.0  18.200000   21\n",
      "148     2  120.0  26.800000   27\n",
      "514     2   91.0  27.300000   22\n",
      "387     1  126.0  30.100000   47\n",
      "272     1   79.0  32.000000   22\n",
      "162     2  124.0  32.900000   30\n",
      "360     5  143.0  45.000000   47\n",
      "133     0  102.0  34.500000   24\n",
      "92      2   93.0  38.000000   23\n",
      "361    10   75.0  33.300000   38\n",
      "575     2  122.0  35.900000   26\n",
      "124     2  108.0  25.300000   22\n",
      "105     0  120.0  30.500000   26\n",
      "582    10   68.0  35.500000   47\n",
      "123     4  110.0  28.400000   27\n",
      "353     1  106.0  34.200000   22\n",
      "538    11  143.0  36.600000   51\n",
      "614     8  151.0  42.900000   36\n",
      "700     7   97.0  40.900000   32\n",
      "315     8  133.0  32.900000   39\n",
      "131     0  117.0  45.200000   24\n",
      "126     1  109.0  28.500000   22\n",
      "589     0  125.0  22.500000   21\n",
      "83      3   80.0  34.200000   27\n",
      "217     2  129.0  33.200000   25\n",
      "43     13   76.0  32.800000   41\n",
      "443     0   94.0  31.992578   25\n",
      "243     7  161.0  30.400000   47\n",
      "\n",
      "[614 rows x 4 columns]\n",
      "     preg   plas  mass  age\n",
      "513     6  103.0  37.7   55\n",
      "730     2   99.0  20.4   27\n",
      "602     3  102.0  29.5   32\n",
      "556     5  166.0  25.8   51\n",
      "256     4  134.0  23.8   60\n",
      "484     6  124.0  27.6   29\n",
      "485     8  105.0  43.3   45\n",
      "615     4  109.0  34.8   26\n",
      "93     11  135.0  52.3   40\n",
      "764     4   83.0  29.3   34\n",
      "365     1  119.0  45.3   26\n",
      "20      0  146.0  37.9   28\n",
      "435    10  133.0  27.0   36\n",
      "578     3  148.0  32.5   22\n",
      "383     0  100.0  30.8   21\n",
      "187     1   87.0  37.6   24\n",
      "444     3   96.0  24.7   39\n",
      "130     5  147.0  33.7   65\n",
      "523     5  139.0  28.6   26\n",
      "584     5  106.0  39.5   38\n",
      "704     0  180.0  36.5   35\n",
      "662     3  113.0  29.5   25\n",
      "3       3  107.0  22.9   23\n",
      "612    10  179.0  35.1   37\n",
      "411     0  100.0  46.8   31\n",
      "242     0  179.0  37.8   22\n",
      "380     2  128.0  40.0   24\n",
      "232     0  145.0  44.2   31\n",
      "140     3   83.0  34.3   25\n",
      "154     2  197.0  30.5   53\n",
      "..    ...    ...   ...  ...\n",
      "748     0   97.0  36.8   25\n",
      "60      5   77.0  35.8   35\n",
      "751     1  193.0  25.9   24\n",
      "176     1  113.0  33.6   21\n",
      "208     4   96.0  20.8   26\n",
      "73      7   83.0  29.3   36\n",
      "259     6  117.0  28.7   30\n",
      "377     8   99.0  35.4   50\n",
      "442     2  125.0  33.8   31\n",
      "724    10  122.0  27.6   45\n",
      "316     6  194.0  23.5   59\n",
      "104     7  136.0  29.9   50\n",
      "609     2   56.0  24.2   22\n",
      "40      2  112.0  34.1   26\n",
      "323     0  137.0  43.1   33\n",
      "532     3  116.0  26.3   24\n",
      "282     2   75.0  29.7   33\n",
      "136     3  111.0  30.1   30\n",
      "266     3  158.0  31.2   24\n",
      "245    12   84.0  29.7   46\n",
      "652     1  116.0  36.1   25\n",
      "498     2   89.0  33.5   42\n",
      "265     0  161.0  21.9   65\n",
      "450    10  115.0  35.3   29\n",
      "758     1   86.0  41.3   29\n",
      "374     8  100.0  38.7   42\n",
      "372     1  114.0  38.1   21\n",
      "490     6  151.0  35.5   28\n",
      "138     2   92.0  24.2   28\n",
      "399     4  144.0  38.5   37\n",
      "\n",
      "[154 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "# Obtenemos train y test _atts con las mejores variables, se usará más adelante en los ensembles\n",
    "(train_attsVar, test_attsVar) = selectK(mutual_info_classif, 4, df[1], train_atts[1], train_label[1], test_atts[1])\n",
    "print(train_attsVar)\n",
    "print(test_attsVar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se han elegido las 4 mejores variables dado que tenemos 8 variables predictoras en total, y no queríamos usar menos para no dar pie a perder mucha información.\n",
    "Ahora haremos una primera prueba con nuestro ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.72727272727272729"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clasif=ensemble(train_attsVar,train_label[1], test_attsVar, test_label[1], 100,True,150,seed,5,0.5)\n",
    "metrics.accuracy_score(test_label[1],clasif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Resultado obtenido sin aplicar filter: 0.70779220779220775\n",
    "- Resultado obtenido aplicando filter: 0.72727272727272729\n",
    "\n",
    "En el caso de Pima mejora, en parte tiene su lógica debido a que el accuracy era bajo, por ello es sencillo mejorarlo. \n",
    "Al elegir las variables que tienen mayor ganancia obtenemos una mejor predicción."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora vamos a ver la influencia que tiene al aplicar filter.\n",
    "Para ello vamos a ver si obtenemos mejora con respecto al clasificador que ha obtenido uno de los mejores accuracys y uno de los peores.\n",
    "\n",
    "Uno de los mejores accuracy Pima (RandomForest):\n",
    "\n",
    "Configuracion más óptima: \n",
    "- Accuracy :  0.798147090551\n",
    "- Criterion:  gini\n",
    "- N_estimators:  18\n",
    "- Max_depth:  5\n",
    "- Min_samples_split:  2\n",
    "- Min_samples_leaf:  3\n",
    "- Bootstrap:  True\n",
    "- Random_state:  1234\n",
    "- Accuracy de test: 0.70779220779220775\n",
    "\n",
    "Uno de los peores accuracy Pima (Boosting):\n",
    "\n",
    "Configuracion más óptima: \n",
    "- Accuracy :  0.794831524843\n",
    "- Estimator:  DecisionTreeClassifier(\n",
    "                        class_weight=None, criterion='gini', max_depth=5,\n",
    "                        max_features=None, max_leaf_nodes=None,\n",
    "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "                        min_samples_leaf=1, min_samples_split=2,\n",
    "                        min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
    "                        splitter='best')\n",
    "- N_estimators:  98\n",
    "- Learning_rate:  0.8999999999999999\n",
    "- Random_state:  1234\n",
    "- Accuracy de test: 0.6428571428571429"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.68181818181818177"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf = RandomForestClassifier(n_estimators = 18, \n",
    "                                criterion=\"gini\", \n",
    "                                max_depth=5,\n",
    "                                min_samples_split = 2,\n",
    "                                min_samples_leaf = 3,\n",
    "                                bootstrap = True,\n",
    "                                random_state = seed)\n",
    "\n",
    "\n",
    "rf.fit(train_attsVar, train_label[1])\n",
    "prediction = rf.predict(test_attsVar)\n",
    "metrics.accuracy_score(test_label[1], prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el caso del mejor Accuracy de test, empeoramos dado que perdemos información eliminando las variables de menor importancia, además mejorar una de las mejores configuraciones es más difícil."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.67532467532467533"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boost = AdaBoostClassifier(base_estimator=tree.DecisionTreeClassifier(max_depth=5), \n",
    "                      n_estimators = 98,\n",
    "                      learning_rate = 0.89999,\n",
    "                      random_state=seed)\n",
    "\n",
    "\n",
    "boost.fit(train_attsVar, train_label[1])\n",
    "prediction = boost.predict(test_attsVar)\n",
    "metrics.accuracy_score(test_label[1], prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En cuanto al de peor configuración, obtenemos una mejora, esto es debido dado que al quedarnos con las variables de más importancia, obtenemos una mejor predicción, además de que mejorar la peor configuración es más sencillo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1.1.2 Método filter basado en rankings con dataset Wisconsin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.48603098768879649, 'cellSize'),\n",
       " (0.46993766833372019, 'CellShape'),\n",
       " (0.41167880380619892, 'bareNuclei'),\n",
       " (0.37911994505258395, 'blandChromatin'),\n",
       " (0.35302212105383335, 'epithelialSize'),\n",
       " (0.31501779705255006, 'clumpThickness'),\n",
       " (0.31169836388044914, 'normalNucleoli'),\n",
       " (0.30767797478207859, 'marginalAdhesion'),\n",
       " (0.14952150918842766, 'mitoses')]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrica=mutual_info_classif(train_atts[2], train_label[2], random_state=seed)\n",
    "filterRank(metrica, df[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variables seleccionadas: \n",
      "cellSize\n",
      "CellShape\n",
      "bareNuclei\n",
      "blandChromatin\n",
      "(559, 4)\n",
      "     cellSize  CellShape  bareNuclei  blandChromatin\n",
      "91          1          1    1.000000               3\n",
      "200        10          7    3.536732               8\n",
      "677         1          1    1.000000               1\n",
      "234        10         10   10.000000               7\n",
      "221         1          3    2.000000               2\n",
      "253         1          1    1.000000               2\n",
      "102         3          1    2.000000               5\n",
      "575        10          7    1.000000              10\n",
      "324         1          1    1.000000               3\n",
      "445        10         10   10.000000               6\n",
      "338         1          1    1.000000               2\n",
      "348         1          1    1.000000               2\n",
      "295         1          1    1.000000               2\n",
      "245         3          5   10.000000               7\n",
      "556         1          1    1.000000               2\n",
      "688         2          3    1.000000               1\n",
      "439         1          1    1.000000               2\n",
      "1           4          4   10.000000               3\n",
      "515         1          1    3.000000               2\n",
      "244        10          6   10.000000               4\n",
      "321        10          3   10.000000               5\n",
      "444         1          1    1.000000               2\n",
      "271         3          4    3.536732               4\n",
      "662         4          6   10.000000               7\n",
      "267         1          1    1.000000               3\n",
      "536         3          4   10.000000               4\n",
      "159         1          1    1.000000               2\n",
      "146         1          2    3.536732               1\n",
      "251         1          1    1.000000               3\n",
      "307         1          1    1.000000               1\n",
      "..        ...        ...         ...             ...\n",
      "23          1          1    1.000000               3\n",
      "374         7          8   10.000000               7\n",
      "670         1          1    1.000000               1\n",
      "132         5          5    5.000000               4\n",
      "176         5         10   10.000000               4\n",
      "220         4          3   10.000000               5\n",
      "96          1          2    1.000000               3\n",
      "122         1          1    1.000000               3\n",
      "489         1          1    2.000000               2\n",
      "504         2          2    1.000000               2\n",
      "394         1          1    1.000000               1\n",
      "40         10         10   10.000000               7\n",
      "585        10         10    2.000000              10\n",
      "368         1          1    1.000000               2\n",
      "362         1          1    4.000000               1\n",
      "506         1          1    1.000000               2\n",
      "78          5          7    9.000000               7\n",
      "58         10         10    8.000000               3\n",
      "449         2          1    1.000000               1\n",
      "312         4          4    1.000000               3\n",
      "528         1          1    1.000000               1\n",
      "226         1          1    2.000000               3\n",
      "319        10          5    4.000000               4\n",
      "508         1          1    1.000000               3\n",
      "7           1          2    1.000000               3\n",
      "130         1          1    1.000000               1\n",
      "559         1          1    3.536732               1\n",
      "697         1          1    1.000000               1\n",
      "656         4         10    4.000000               7\n",
      "612         1          1    1.000000               1\n",
      "\n",
      "[559 rows x 4 columns]\n",
      "     cellSize  CellShape  bareNuclei  blandChromatin\n",
      "180         1          1    1.000000               3\n",
      "403         1          1    1.000000               1\n",
      "56          5          5    2.000000               5\n",
      "464         6          7   10.000000               8\n",
      "659         1          1    5.000000               5\n",
      "414         1          2    1.000000               1\n",
      "21          1          1    1.000000               2\n",
      "260         4          5   10.000000               3\n",
      "391         9          7    8.000000               4\n",
      "230         1          1    3.536732               3\n",
      "22          4          5    3.536732               7\n",
      "472         5          6   10.000000               5\n",
      "283         1          1    1.000000               1\n",
      "134         1          3    3.536732               2\n",
      "614        10         10   10.000000              10\n",
      "666         1          1    1.000000               3\n",
      "576         2          4    2.000000               2\n",
      "27          1          1    1.000000               2\n",
      "698         1          1    2.000000               1\n",
      "589         1          1    1.000000               2\n",
      "682        10          8    5.000000              10\n",
      "247         1          1    1.000000               3\n",
      "145         1          1    1.000000               3\n",
      "522         1          1    1.000000               2\n",
      "46          8          7    8.000000               3\n",
      "53         10         10    6.000000               3\n",
      "144         2          2    1.000000               2\n",
      "258         4          3   10.000000               5\n",
      "87          1          1    1.000000               2\n",
      "529         1          1    1.000000               1\n",
      "..        ...        ...         ...             ...\n",
      "77          1          1    2.000000               7\n",
      "466         1          1    1.000000               1\n",
      "477         1          3    1.000000               1\n",
      "582         1          1    1.000000               1\n",
      "647         1          1    1.000000               3\n",
      "155         1          1    1.000000               3\n",
      "609         1          4    1.000000               1\n",
      "115         4          6    7.000000               8\n",
      "105        10         10    8.000000               8\n",
      "216         1          4    3.536732               3\n",
      "273         1          2   10.000000               7\n",
      "530        10         10   10.000000              10\n",
      "204         1          1    5.000000               2\n",
      "85          1          1    1.000000               1\n",
      "235         6          6   10.000000               3\n",
      "526        10          5   10.000000               6\n",
      "322         2          2    3.000000               2\n",
      "503         1          1    1.000000               1\n",
      "478         1          1    1.000000               2\n",
      "289         4          4    5.000000               7\n",
      "57          3          5    3.000000               4\n",
      "634         4          5   10.000000               4\n",
      "325         1          1    1.000000               2\n",
      "652         1          1    1.000000               3\n",
      "149         1          1    1.000000               3\n",
      "196        10         10   10.000000              10\n",
      "448         1          1    1.000000               1\n",
      "512         1          1    1.000000               3\n",
      "25          2          1    1.000000               2\n",
      "11          1          1    1.000000               2\n",
      "\n",
      "[140 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "# Obtenemos train y test _atts con las mejores variables, se usará más adelante en los ensembles\n",
    "(train_attsVar2, test_attsVar2) = selectK(mutual_info_classif, 4, df[2], train_atts[2], train_label[2], test_atts[2])\n",
    "print(train_attsVar2)\n",
    "print(test_attsVar2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este caso elegimos las mejores 4 variables por la misma explicación de antes.\n",
    "Ahora haremos una prueba con nuestro ensemble."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.94285714285714284"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clasif=ensemble(train_attsVar2,train_label[2], test_attsVar2, test_label[2], 100,True,150,seed,5,0.5)\n",
    "metrics.accuracy_score(test_label[2],clasif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Resultado obtenido sin aplicar filter: 0.94999999999999996\n",
    "- Resultado obtenido aplicando filter: 0.94285714285714284\n",
    "\n",
    "En este caso, dado que la primera prediccion sin utilizar filter es muy buena, lo que estamos haciendo al eliminar las variables de menor ganancia, es perder conocimiento específico."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora vamos a ver la influencia que tiene aplicar filter.\n",
    "Para ello vamos a ver si obtenemos mejora con respecto al clasificador que ha obtenido el mejor accuracy y el peor.\n",
    "\n",
    "Uno de los mejores accuracy Wisconsin (Bagging):\n",
    "\n",
    "Configuracion más óptima: \n",
    "- Accuracy :  0.969610718188\n",
    "- Estimator:  DecisionTreeClassifier(\n",
    "                        class_weight=None, criterion='gini', max_depth=None,\n",
    "                        max_features=None, max_leaf_nodes=None,\n",
    "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "                        min_samples_leaf=1, min_samples_split=2,\n",
    "                        min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
    "                        splitter='best')\n",
    "- N_estimators:  11\n",
    "- Max_Samples:  0.6\n",
    "- Bootstrap:  True\n",
    "- Random_state:  1234\n",
    "- Accuracy de test: 0.95714285714285718\n",
    "\n",
    "\n",
    "Uno de los peores accuracy Wisconsin (RandomForest):\n",
    "\n",
    "Configuracion más óptima: \n",
    "- Accuracy :  0.973175780576\n",
    "- Criterion:  gini\n",
    "- N_estimators:  13\n",
    "- Max_depth:  None\n",
    "- Min_samples_split:  3\n",
    "- Min_samples_leaf:  1\n",
    "- Bootstrap:  True\n",
    "- Random_state:  1234  \n",
    "- Accuracy de test: 0.93571428571428572"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.90714285714285714"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bagg = BaggingClassifier(tree.DecisionTreeClassifier(), \n",
    "                          n_estimators = 11,\n",
    "                          max_samples = 0.6,\n",
    "                          bootstrap = True,\n",
    "                          random_state=seed)\n",
    "\n",
    "\n",
    "bagg.fit(train_attsVar2, train_label[2])\n",
    "prediction = bagg.predict(test_attsVar2)\n",
    "metrics.accuracy_score(test_label[2], prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.93571428571428572"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf = RandomForestClassifier(n_estimators = 13, \n",
    "                                criterion=\"gini\", \n",
    "                                max_depth=None,\n",
    "                                min_samples_split = 3,\n",
    "                                min_samples_leaf = 1,\n",
    "                                bootstrap = True,\n",
    "                                random_state = seed)\n",
    "\n",
    "\n",
    "rf.fit(train_attsVar2, train_label[2])\n",
    "prediction = rf.predict(test_attsVar2)\n",
    "metrics.accuracy_score(test_label[2], prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En estos casos, dado que obtenemos un accuracy tan alto en las distintas configuraciones (ya sea el más alto o el más bajo), es muy difícil mejorarlo, por lo tanto al eliminar las variables menos importantes, perdemos información."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.2 Método filter basado en la importancia de las variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función que retorna un ranking con la importancia de cada una de las variables al aplicar un determinado\n",
    "# para ello se el indica el clasificador y df [solo usado para obtener las variables predictoras]\n",
    "def filterImportance(classifer, df):\n",
    "    scores = classifier.feature_importances_\n",
    "    names = list(df.keys())\n",
    "    ranks = sorted( list(zip(scores, names)), reverse=True )\n",
    "    # Se retorna una lista con las variables y su Score\n",
    "    return ranks\n",
    "\n",
    "# Función que retorna un par de diccionarios (train_atts y test_atts) eliminando las variables con menor Score,\n",
    "# se indicará por parámetro el modelo a emplear, forma de evaluar la importancia de las variables [threshold], \n",
    "# el dataset completo [solo usado para obtener el conjunto de variables predictoras], train y test\n",
    "def selectModel(model,threshold,train_atts, train_label, test_atts, df):\n",
    "    # Se obtienen las variables a salvar tras aplicar el método SelectFromModel y los parámetros datos por el \n",
    "    # usuario\n",
    "    # Nota: fss.get_support se obtiene una lista con True's y False's indicando que variables se salvan o no, están\n",
    "    # en orden con respecto a train_atts\n",
    "    fss = SelectFromModel(estimator = model, threshold = threshold).fit(train_atts, train_label)\n",
    "    train_attsCopy = train_atts.copy()\n",
    "    test_attsCopy = test_atts.copy()\n",
    "    print(\"Variables seleccionadas: \")\n",
    "    # Se mira que variables se salva, las que se salvan se muestran en pantalla, las que no se eliminan de train\n",
    "    # y test (se usa también test para evitar problemas a la hora de usar predicción más adelante)\n",
    "    for i in range(0,len(fss.get_support())):\n",
    "        if (fss.get_support())[i]:\n",
    "            print((df.keys())[i])\n",
    "        else:\n",
    "            del train_attsCopy[(df.keys())[i]]\n",
    "            del test_attsCopy[(df.keys())[i]]\n",
    "    print(train_attsCopy.shape)\n",
    "    return (train_attsCopy, test_attsCopy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1.2.1 Método filter basado en la importancia de las variables con dataset Pima"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ejecutando el algoritmo de filter basado en importancia de variables, vamos a obtener la lista de las variables ordenado por su importancia. Para ello, usaremos un clasificador y nos dirá cuales son las variables con la mayor importancia."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este caso, dado que nuestro ensemble únicamente retorna un vector con todos los casos clasificados, no podemos devolver un modelo, y por lo tanto no podemos ejecutar este método filter. Por ello, vamos a proceder a ejecutar una de las mejores y una de las peores configuraciones y veremos si hay mejora o no."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora vamos a ver la influencia que tiene aplicar filter.\n",
    "Para ello vamos a ver si obtenemos mejora con respecto al clasificador que ha obtenido uno de los mejores accuracys y uno de los peores.\n",
    "\n",
    "Uno de los mejores accuracy Pima (RandomForest):\n",
    "\n",
    "Configuracion más óptima: \n",
    "- Accuracy :  0.798147090551\n",
    "- Criterion:  gini\n",
    "- N_estimators:  18\n",
    "- Max_depth:  5\n",
    "- Min_samples_split:  2\n",
    "- Min_samples_leaf:  3\n",
    "- Bootstrap:  True\n",
    "- Random_state:  1234\n",
    "- Accuracy de test: 0.70779220779220775\n",
    "\n",
    "Uno de los peores accuracy Pima (Boosting):\n",
    "\n",
    "Configuracion más óptima: \n",
    "- Accuracy :  0.794831524843\n",
    "- Estimator:  DecisionTreeClassifier(\n",
    "                        class_weight=None, criterion='gini', max_depth=5,\n",
    "                        max_features=None, max_leaf_nodes=None,\n",
    "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "                        min_samples_leaf=1, min_samples_split=2,\n",
    "                        min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
    "                        splitter='best')\n",
    "- N_estimators:  98\n",
    "- Learning_rate:  0.8999999999999999\n",
    "- Random_state:  1234\n",
    "- Accuracy de test: 0.6428571428571429"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.31522276881552447, 'plas'), (0.22635369791194165, 'mass'), (0.17287380555691637, 'age'), (0.076229757919770116, 'pedi'), (0.061763106639289252, 'insu'), (0.061046738762102454, 'preg'), (0.050701348379060464, 'pres'), (0.035808776015395227, 'skin')]\n"
     ]
    }
   ],
   "source": [
    "# Modelo\n",
    "rf = RandomForestClassifier(n_estimators = 18, \n",
    "                                criterion=\"gini\", \n",
    "                                max_depth=5,\n",
    "                                min_samples_split = 2,\n",
    "                                min_samples_leaf = 3,\n",
    "                                bootstrap = True,\n",
    "                                random_state = seed)\n",
    "\n",
    "\n",
    "classifier = rf.fit(train_atts[1], train_label[1])\n",
    "ranks = filterImportance(classifier, df[1])\n",
    "print(ranks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variables seleccionadas: \n",
      "plas\n",
      "mass\n",
      "age\n",
      "(614, 3)\n",
      "      plas       mass  age\n",
      "54    74.0  27.800000   22\n",
      "156  183.0  23.300000   32\n",
      "38   108.0  27.100000   24\n",
      "646  101.0  21.800000   22\n",
      "447   97.0  28.200000   22\n",
      "759  109.0  25.400000   21\n",
      "576  101.0  35.700000   26\n",
      "660  189.0  28.500000   37\n",
      "431   71.0  28.000000   22\n",
      "568  155.0  38.700000   34\n",
      "486  124.0  28.700000   52\n",
      "388  145.0  30.300000   53\n",
      "539  102.0  30.800000   36\n",
      "355  109.0  36.000000   60\n",
      "51   125.0  30.000000   32\n",
      "525  139.0  22.100000   21\n",
      "421  113.0  31.000000   21\n",
      "557  184.0  30.000000   49\n",
      "691  108.0  32.400000   42\n",
      "395  125.0  27.600000   49\n",
      "61   137.0  24.200000   55\n",
      "598  130.0  25.900000   22\n",
      "410  108.0  35.500000   24\n",
      "327  196.0  39.800000   41\n",
      "122   90.0  25.100000   25\n",
      "430   92.0  31.600000   24\n",
      "417  108.0  26.000000   25\n",
      "66   106.0  36.600000   45\n",
      "160  136.0  28.300000   42\n",
      "590  135.0  40.600000   26\n",
      "..     ...        ...  ...\n",
      "86   188.0  32.000000   22\n",
      "642  122.0  23.000000   40\n",
      "494   97.0  18.200000   21\n",
      "148  120.0  26.800000   27\n",
      "514   91.0  27.300000   22\n",
      "387  126.0  30.100000   47\n",
      "272   79.0  32.000000   22\n",
      "162  124.0  32.900000   30\n",
      "360  143.0  45.000000   47\n",
      "133  102.0  34.500000   24\n",
      "92    93.0  38.000000   23\n",
      "361   75.0  33.300000   38\n",
      "575  122.0  35.900000   26\n",
      "124  108.0  25.300000   22\n",
      "105  120.0  30.500000   26\n",
      "582   68.0  35.500000   47\n",
      "123  110.0  28.400000   27\n",
      "353  106.0  34.200000   22\n",
      "538  143.0  36.600000   51\n",
      "614  151.0  42.900000   36\n",
      "700   97.0  40.900000   32\n",
      "315  133.0  32.900000   39\n",
      "131  117.0  45.200000   24\n",
      "126  109.0  28.500000   22\n",
      "589  125.0  22.500000   21\n",
      "83    80.0  34.200000   27\n",
      "217  129.0  33.200000   25\n",
      "43    76.0  32.800000   41\n",
      "443   94.0  31.992578   25\n",
      "243  161.0  30.400000   47\n",
      "\n",
      "[614 rows x 3 columns]\n",
      "      plas  mass  age\n",
      "513  103.0  37.7   55\n",
      "730   99.0  20.4   27\n",
      "602  102.0  29.5   32\n",
      "556  166.0  25.8   51\n",
      "256  134.0  23.8   60\n",
      "484  124.0  27.6   29\n",
      "485  105.0  43.3   45\n",
      "615  109.0  34.8   26\n",
      "93   135.0  52.3   40\n",
      "764   83.0  29.3   34\n",
      "365  119.0  45.3   26\n",
      "20   146.0  37.9   28\n",
      "435  133.0  27.0   36\n",
      "578  148.0  32.5   22\n",
      "383  100.0  30.8   21\n",
      "187   87.0  37.6   24\n",
      "444   96.0  24.7   39\n",
      "130  147.0  33.7   65\n",
      "523  139.0  28.6   26\n",
      "584  106.0  39.5   38\n",
      "704  180.0  36.5   35\n",
      "662  113.0  29.5   25\n",
      "3    107.0  22.9   23\n",
      "612  179.0  35.1   37\n",
      "411  100.0  46.8   31\n",
      "242  179.0  37.8   22\n",
      "380  128.0  40.0   24\n",
      "232  145.0  44.2   31\n",
      "140   83.0  34.3   25\n",
      "154  197.0  30.5   53\n",
      "..     ...   ...  ...\n",
      "748   97.0  36.8   25\n",
      "60    77.0  35.8   35\n",
      "751  193.0  25.9   24\n",
      "176  113.0  33.6   21\n",
      "208   96.0  20.8   26\n",
      "73    83.0  29.3   36\n",
      "259  117.0  28.7   30\n",
      "377   99.0  35.4   50\n",
      "442  125.0  33.8   31\n",
      "724  122.0  27.6   45\n",
      "316  194.0  23.5   59\n",
      "104  136.0  29.9   50\n",
      "609   56.0  24.2   22\n",
      "40   112.0  34.1   26\n",
      "323  137.0  43.1   33\n",
      "532  116.0  26.3   24\n",
      "282   75.0  29.7   33\n",
      "136  111.0  30.1   30\n",
      "266  158.0  31.2   24\n",
      "245   84.0  29.7   46\n",
      "652  116.0  36.1   25\n",
      "498   89.0  33.5   42\n",
      "265  161.0  21.9   65\n",
      "450  115.0  35.3   29\n",
      "758   86.0  41.3   29\n",
      "374  100.0  38.7   42\n",
      "372  114.0  38.1   21\n",
      "490  151.0  35.5   28\n",
      "138   92.0  24.2   28\n",
      "399  144.0  38.5   37\n",
      "\n",
      "[154 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "(train_attsVar, test_attsVar) = selectModel(rf,\"mean\",train_atts[1], train_label[1], test_atts[1], df[1])\n",
    "print (train_attsVar)\n",
    "print (test_attsVar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.69480519480519476"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf.fit(train_attsVar, train_label[1])\n",
    "prediction = rf.predict(test_attsVar)\n",
    "metrics.accuracy_score(test_label[1], prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el caso del mejor Accuracy de test, empeoramos dado que perdemos información eliminando las variables de menor importancia, además mejorar una de las mejores configuraciones es más difícil."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora, vamos a comprobar que ocurre con uno de los que peor Accuracy han obtenido."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.18175892439044469, 'pedi'), (0.18005259886452382, 'plas'), (0.15669191820380915, 'mass'), (0.13925013895184199, 'age'), (0.13109894414090861, 'pres'), (0.085254529404819077, 'preg'), (0.075250414579007954, 'skin'), (0.050642531464644709, 'insu')]\n"
     ]
    }
   ],
   "source": [
    "# Modelo\n",
    "boost = AdaBoostClassifier(base_estimator=tree.DecisionTreeClassifier(max_depth=5), \n",
    "                      n_estimators = 98,\n",
    "                      learning_rate = 0.89999,\n",
    "                      random_state=seed)\n",
    "\n",
    "\n",
    "classifier = boost.fit(train_atts[1], train_label[1])\n",
    "ranks = filterImportance(classifier, df[1])\n",
    "print(ranks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variables seleccionadas: \n",
      "plas\n",
      "pres\n",
      "mass\n",
      "pedi\n",
      "age\n",
      "(614, 5)\n",
      "      plas        pres       mass   pedi  age\n",
      "54    74.0   52.000000  27.800000  0.269   22\n",
      "156  183.0   64.000000  23.300000  0.672   32\n",
      "38   108.0   88.000000  27.100000  0.400   24\n",
      "646  101.0   58.000000  21.800000  0.155   22\n",
      "447   97.0   60.000000  28.200000  0.443   22\n",
      "759  109.0   60.000000  25.400000  0.947   21\n",
      "576  101.0   76.000000  35.700000  0.198   26\n",
      "660  189.0  110.000000  28.500000  0.680   37\n",
      "431   71.0   70.000000  28.000000  0.586   22\n",
      "568  155.0   84.000000  38.700000  0.619   34\n",
      "486  124.0   76.000000  28.700000  0.687   52\n",
      "388  145.0   88.000000  30.300000  0.771   53\n",
      "539  102.0   82.000000  30.800000  0.180   36\n",
      "355  109.0   75.000000  36.000000  0.546   60\n",
      "51   125.0   68.000000  30.000000  0.464   32\n",
      "525  139.0   62.000000  22.100000  0.207   21\n",
      "421  113.0   80.000000  31.000000  0.874   21\n",
      "557  184.0   85.000000  30.000000  1.213   49\n",
      "691  108.0   66.000000  32.400000  0.272   42\n",
      "395  125.0   78.000000  27.600000  0.565   49\n",
      "61   137.0   61.000000  24.200000  0.151   55\n",
      "598  130.0   70.000000  25.900000  0.472   22\n",
      "410  108.0   60.000000  35.500000  0.415   24\n",
      "327  196.0   90.000000  39.800000  0.451   41\n",
      "122   90.0   62.000000  25.100000  1.268   25\n",
      "430   92.0   62.000000  31.600000  0.130   24\n",
      "417  108.0   62.000000  26.000000  0.223   25\n",
      "66   106.0   72.000000  36.600000  0.178   45\n",
      "160  136.0   84.000000  28.300000  0.260   42\n",
      "590  135.0   94.000000  40.600000  0.284   26\n",
      "..     ...         ...        ...    ...  ...\n",
      "86   188.0   82.000000  32.000000  0.682   22\n",
      "642  122.0   78.000000  23.000000  0.254   40\n",
      "494   97.0   70.000000  18.200000  0.147   21\n",
      "148  120.0   54.000000  26.800000  0.455   27\n",
      "514   91.0   62.000000  27.300000  0.525   22\n",
      "387  126.0   60.000000  30.100000  0.349   47\n",
      "272   79.0   75.000000  32.000000  0.396   22\n",
      "162  124.0   68.000000  32.900000  0.875   30\n",
      "360  143.0   78.000000  45.000000  0.190   47\n",
      "133  102.0   78.000000  34.500000  0.238   24\n",
      "92    93.0   64.000000  38.000000  0.674   23\n",
      "361   75.0   82.000000  33.300000  0.263   38\n",
      "575  122.0   76.000000  35.900000  0.483   26\n",
      "124  108.0   62.000000  25.300000  0.881   22\n",
      "105  120.0   74.000000  30.500000  0.285   26\n",
      "582   68.0  106.000000  35.500000  0.285   47\n",
      "123  110.0   76.000000  28.400000  0.118   27\n",
      "353  106.0   70.000000  34.200000  0.142   22\n",
      "538  143.0   94.000000  36.600000  0.254   51\n",
      "614  151.0   78.000000  42.900000  0.516   36\n",
      "700   97.0   76.000000  40.900000  0.871   32\n",
      "315  133.0   72.000000  32.900000  0.270   39\n",
      "131  117.0   80.000000  45.200000  0.089   24\n",
      "126  109.0   58.000000  28.500000  0.219   22\n",
      "589  125.0   96.000000  22.500000  0.262   21\n",
      "83    80.0   82.000000  34.200000  1.292   27\n",
      "217  129.0   74.000000  33.200000  0.591   25\n",
      "43    76.0   60.000000  32.800000  0.180   41\n",
      "443   94.0   69.105469  31.992578  0.256   25\n",
      "243  161.0   86.000000  30.400000  0.165   47\n",
      "\n",
      "[614 rows x 5 columns]\n",
      "      plas        pres  mass   pedi  age\n",
      "513  103.0   72.000000  37.7  0.324   55\n",
      "730   99.0   70.000000  20.4  0.235   27\n",
      "602  102.0   74.000000  29.5  0.121   32\n",
      "556  166.0   72.000000  25.8  0.587   51\n",
      "256  134.0   72.000000  23.8  0.277   60\n",
      "484  124.0   72.000000  27.6  0.368   29\n",
      "485  105.0  100.000000  43.3  0.239   45\n",
      "615  109.0   64.000000  34.8  0.905   26\n",
      "93   135.0   69.105469  52.3  0.578   40\n",
      "764   83.0   86.000000  29.3  0.317   34\n",
      "365  119.0   88.000000  45.3  0.507   26\n",
      "20   146.0   70.000000  37.9  0.334   28\n",
      "435  133.0   68.000000  27.0  0.245   36\n",
      "578  148.0   66.000000  32.5  0.256   22\n",
      "383  100.0   70.000000  30.8  0.597   21\n",
      "187   87.0   68.000000  37.6  0.401   24\n",
      "444   96.0   56.000000  24.7  0.944   39\n",
      "130  147.0   78.000000  33.7  0.218   65\n",
      "523  139.0   64.000000  28.6  0.411   26\n",
      "584  106.0   82.000000  39.5  0.286   38\n",
      "704  180.0   90.000000  36.5  0.314   35\n",
      "662  113.0   50.000000  29.5  0.626   25\n",
      "3    107.0   62.000000  22.9  0.678   23\n",
      "612  179.0   70.000000  35.1  0.200   37\n",
      "411  100.0   88.000000  46.8  0.962   31\n",
      "242  179.0   50.000000  37.8  0.455   22\n",
      "380  128.0   64.000000  40.0  1.101   24\n",
      "232  145.0   69.105469  44.2  0.630   31\n",
      "140   83.0   58.000000  34.3  0.336   25\n",
      "154  197.0   70.000000  30.5  0.158   53\n",
      "..     ...         ...   ...    ...  ...\n",
      "748   97.0   64.000000  36.8  0.600   25\n",
      "60    77.0   82.000000  35.8  0.156   35\n",
      "751  193.0   50.000000  25.9  0.655   24\n",
      "176  113.0   64.000000  33.6  0.543   21\n",
      "208   96.0   56.000000  20.8  0.340   26\n",
      "73    83.0   78.000000  29.3  0.767   36\n",
      "259  117.0   96.000000  28.7  0.157   30\n",
      "377   99.0   84.000000  35.4  0.388   50\n",
      "442  125.0   60.000000  33.8  0.088   31\n",
      "724  122.0   78.000000  27.6  0.512   45\n",
      "316  194.0   78.000000  23.5  0.129   59\n",
      "104  136.0   90.000000  29.9  0.210   50\n",
      "609   56.0   56.000000  24.2  0.332   22\n",
      "40   112.0   68.000000  34.1  0.315   26\n",
      "323  137.0   40.000000  43.1  2.288   33\n",
      "532  116.0   74.000000  26.3  0.107   24\n",
      "282   75.0   64.000000  29.7  0.370   33\n",
      "136  111.0   56.000000  30.1  0.557   30\n",
      "266  158.0   64.000000  31.2  0.295   24\n",
      "245   84.0   72.000000  29.7  0.297   46\n",
      "652  116.0   78.000000  36.1  0.496   25\n",
      "498   89.0   90.000000  33.5  0.292   42\n",
      "265  161.0   50.000000  21.9  0.254   65\n",
      "450  115.0   69.105469  35.3  0.134   29\n",
      "758   86.0   66.000000  41.3  0.917   29\n",
      "374  100.0   76.000000  38.7  0.190   42\n",
      "372  114.0   66.000000  38.1  0.289   21\n",
      "490  151.0   62.000000  35.5  0.692   28\n",
      "138   92.0   76.000000  24.2  1.698   28\n",
      "399  144.0   82.000000  38.5  0.554   37\n",
      "\n",
      "[154 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "(train_attsVar, test_attsVar) = selectModel(boost,\"mean\",train_atts[1], train_label[1], test_atts[1], df[1])\n",
    "print (train_attsVar)\n",
    "print (test_attsVar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7142857142857143"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boost.fit(train_attsVar, train_label[1])\n",
    "prediction = boost.predict(test_attsVar)\n",
    "metrics.accuracy_score(test_label[1], prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En cuanto al de peor configuración, obtenemos una mejora, esto es debido dado que al quedarnos con las variables de más importancia, obtenemos una mejor predicción, además de que mejorar la peor configuración es más sencillo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1.2.2 Método filter basado en la importancia de las variables con dataset Wisconsin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ejecutando el algoritmo de filter basado en importancia de variables, vamos a obtener la lista de las variables ordenado por su importancia. Para ello, usaremos un clasificador y nos dirá cuales son las variables con la mayor importancia."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este caso, dado que nuestro ensemble únicamente retorna un vector con todos los casos clasificados, no podemos devolver un modelo, y por lo tanto no podemos ejecutar este método filter. Por ello, vamos a proceder a ejecutar una de las mejores y una de las peores configuraciones y veremos si hay mejora o no."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora vamos a ver la influencia que tiene aplicar filter.\n",
    "Para ello vamos a ver si obtenemos mejora con respecto al clasificador que ha obtenido el mejor accuracy y el peor.\n",
    "\n",
    "Uno de los mejores accuracy Wisconsin (Bagging):\n",
    "\n",
    "Configuracion más óptima: \n",
    "- Accuracy :  0.969610718188\n",
    "- Estimator:  DecisionTreeClassifier(\n",
    "                        class_weight=None, criterion='gini', max_depth=None,\n",
    "                        max_features=None, max_leaf_nodes=None,\n",
    "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "                        min_samples_leaf=1, min_samples_split=2,\n",
    "                        min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
    "                        splitter='best')\n",
    "- N_estimators:  11\n",
    "- Max_Samples:  0.6\n",
    "- Bootstrap:  True\n",
    "- Random_state:  1234\n",
    "- Accuracy de test: 0.95714285714285718\n",
    "\n",
    "\n",
    "Uno de los peores accuracy Wisconsin (RandomForest):\n",
    "\n",
    "Configuracion más óptima: \n",
    "- Accuracy :  0.973175780576\n",
    "- Criterion:  gini\n",
    "- N_estimators:  13\n",
    "- Max_depth:  None\n",
    "- Min_samples_split:  3\n",
    "- Min_samples_leaf:  1\n",
    "- Bootstrap:  True\n",
    "- Random_state:  1234\n",
    "- Accuracy de test: 0.93571428571428572"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelo\n",
    "#bagg = BaggingClassifier(tree.DecisionTreeClassifier(), \n",
    "#                          n_estimators = 11,\n",
    "#                          max_samples = 0.6,\n",
    "#                          bootstrap = True,\n",
    "#                          random_state=seed)\n",
    "\n",
    "\n",
    "#classifier = bagg.fit(train_atts[2], train_label[2])\n",
    "#ranks = filterImportance(classifier, df[2])\n",
    "#print(ranks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#(train_attsVar2, test_attsVar2) = selectModel(bagg,\"mean\",train_atts[2], train_label[2], test_atts[2], df[2])\n",
    "#print (train_attsVar2)\n",
    "#print (test_attsVar2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bagg.fit(train_attsVar2, train_label[2])\n",
    "#prediction = bagg.predict(test_attsVar2)\n",
    "#metrics.accuracy_score(test_label[2], prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este caso, dado que Bagging puede ser usado con muchos estimadores base, no presenta el atributo \"features_importances\", y por lo tanto, no puede ser ejectado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora, veremos que ocurre con la configuración que obtiene un peor Accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.34851923334527224, 'cellSize'), (0.15578243752169424, 'CellShape'), (0.14212707564938676, 'blandChromatin'), (0.13831324529848016, 'bareNuclei'), (0.13224663913558732, 'epithelialSize'), (0.031887936819746526, 'clumpThickness'), (0.031693204632165455, 'normalNucleoli'), (0.014574475373896202, 'marginalAdhesion'), (0.0048557522237710768, 'mitoses')]\n"
     ]
    }
   ],
   "source": [
    "# Modelo\n",
    "rf = RandomForestClassifier(n_estimators = 13, \n",
    "                                criterion=\"gini\", \n",
    "                                max_depth=None,\n",
    "                                min_samples_split = 3,\n",
    "                                min_samples_leaf = 1,\n",
    "                                bootstrap = True,\n",
    "                                random_state = seed)\n",
    "\n",
    "\n",
    "classifier = rf.fit(train_atts[2], train_label[2])\n",
    "ranks = filterImportance(classifier, df[2])\n",
    "print(ranks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variables seleccionadas: \n",
      "cellSize\n",
      "CellShape\n",
      "epithelialSize\n",
      "bareNuclei\n",
      "blandChromatin\n",
      "(559, 5)\n",
      "     cellSize  CellShape  epithelialSize  bareNuclei  blandChromatin\n",
      "91          1          1               2    1.000000               3\n",
      "200        10          7               6    3.536732               8\n",
      "677         1          1               2    1.000000               1\n",
      "234        10         10               8   10.000000               7\n",
      "221         1          3               2    2.000000               2\n",
      "253         1          1               2    1.000000               2\n",
      "102         3          1               2    2.000000               5\n",
      "575        10          7               7    1.000000              10\n",
      "324         1          1               2    1.000000               3\n",
      "445        10         10               6   10.000000               6\n",
      "338         1          1               3    1.000000               2\n",
      "348         1          1               2    1.000000               2\n",
      "295         1          1               2    1.000000               2\n",
      "245         3          5               3   10.000000               7\n",
      "556         1          1               1    1.000000               2\n",
      "688         2          3               2    1.000000               1\n",
      "439         1          1               1    1.000000               2\n",
      "1           4          4               7   10.000000               3\n",
      "515         1          1               2    3.000000               2\n",
      "244        10          6               3   10.000000               4\n",
      "321        10          3               6   10.000000               5\n",
      "444         1          1               1    1.000000               2\n",
      "271         3          4               4    3.536732               4\n",
      "662         4          6               5   10.000000               7\n",
      "267         1          1               2    1.000000               3\n",
      "536         3          4               3   10.000000               4\n",
      "159         1          1               2    1.000000               2\n",
      "146         1          2               3    3.536732               1\n",
      "251         1          1               2    1.000000               3\n",
      "307         1          1               2    1.000000               1\n",
      "..        ...        ...             ...         ...             ...\n",
      "23          1          1               2    1.000000               3\n",
      "374         7          8              10   10.000000               7\n",
      "670         1          1               1    1.000000               1\n",
      "132         5          5               4    5.000000               4\n",
      "176         5         10              10   10.000000               4\n",
      "220         4          3               3   10.000000               5\n",
      "96          1          2               2    1.000000               3\n",
      "122         1          1               2    1.000000               3\n",
      "489         1          1               2    2.000000               2\n",
      "504         2          2               1    1.000000               2\n",
      "394         1          1               2    1.000000               1\n",
      "40         10         10               8   10.000000               7\n",
      "585        10         10              10    2.000000              10\n",
      "368         1          1               2    1.000000               2\n",
      "362         1          1               2    4.000000               1\n",
      "506         1          1               2    1.000000               2\n",
      "78          5          7               8    9.000000               7\n",
      "58         10         10              10    8.000000               3\n",
      "449         2          1               2    1.000000               1\n",
      "312         4          4               5    1.000000               3\n",
      "528         1          1               3    1.000000               1\n",
      "226         1          1               2    2.000000               3\n",
      "319        10          5               8    4.000000               4\n",
      "508         1          1               2    1.000000               3\n",
      "7           1          2               2    1.000000               3\n",
      "130         1          1               2    1.000000               1\n",
      "559         1          1               1    3.536732               1\n",
      "697         1          1               2    1.000000               1\n",
      "656         4         10               4    4.000000               7\n",
      "612         1          1               2    1.000000               1\n",
      "\n",
      "[559 rows x 5 columns]\n",
      "     cellSize  CellShape  epithelialSize  bareNuclei  blandChromatin\n",
      "180         1          1               2    1.000000               3\n",
      "403         1          1               2    1.000000               1\n",
      "56          5          5               2    2.000000               5\n",
      "464         6          7               3   10.000000               8\n",
      "659         1          1               2    5.000000               5\n",
      "414         1          2               2    1.000000               1\n",
      "21          1          1               2    1.000000               2\n",
      "260         4          5               2   10.000000               3\n",
      "391         9          7               5    8.000000               4\n",
      "230         1          1               2    3.536732               3\n",
      "22          4          5               2    3.536732               7\n",
      "472         5          6               4   10.000000               5\n",
      "283         1          1               2    1.000000               1\n",
      "134         1          3               2    3.536732               2\n",
      "614        10         10               5   10.000000              10\n",
      "666         1          1               2    1.000000               3\n",
      "576         2          4               2    2.000000               2\n",
      "27          1          1               2    1.000000               2\n",
      "698         1          1               3    2.000000               1\n",
      "589         1          1               2    1.000000               2\n",
      "682        10          8               6    5.000000              10\n",
      "247         1          1               2    1.000000               3\n",
      "145         1          1               2    1.000000               3\n",
      "522         1          1               2    1.000000               2\n",
      "46          8          7               4    8.000000               3\n",
      "53         10         10               3    6.000000               3\n",
      "144         2          2               2    1.000000               2\n",
      "258         4          3               4   10.000000               5\n",
      "87          1          1               2    1.000000               2\n",
      "529         1          1               2    1.000000               1\n",
      "..        ...        ...             ...         ...             ...\n",
      "77          1          1               2    2.000000               7\n",
      "466         1          1               1    1.000000               1\n",
      "477         1          3               2    1.000000               1\n",
      "582         1          1               2    1.000000               1\n",
      "647         1          1               1    1.000000               3\n",
      "155         1          1               2    1.000000               3\n",
      "609         1          4               2    1.000000               1\n",
      "115         4          6               9    7.000000               8\n",
      "105        10         10              10    8.000000               8\n",
      "216         1          4               2    3.536732               3\n",
      "273         1          2               4   10.000000               7\n",
      "530        10         10               6   10.000000              10\n",
      "204         1          1               1    5.000000               2\n",
      "85          1          1               2    1.000000               1\n",
      "235         6          6               4   10.000000               3\n",
      "526        10          5               4   10.000000               6\n",
      "322         2          2               4    3.000000               2\n",
      "503         1          1               2    1.000000               1\n",
      "478         1          1               1    1.000000               2\n",
      "289         4          4               6    5.000000               7\n",
      "57          3          5               3    3.000000               4\n",
      "634         4          5               6   10.000000               4\n",
      "325         1          1               2    1.000000               2\n",
      "652         1          1               2    1.000000               3\n",
      "149         1          1               2    1.000000               3\n",
      "196        10         10               3   10.000000              10\n",
      "448         1          1               1    1.000000               1\n",
      "512         1          1               2    1.000000               3\n",
      "25          2          1               1    1.000000               2\n",
      "11          1          1               2    1.000000               2\n",
      "\n",
      "[140 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "(train_attsVar2, test_attsVar2) = selectModel(rf,\"mean\",train_atts[2], train_label[2], test_atts[2], df[2])\n",
    "print (train_attsVar2)\n",
    "print (test_attsVar2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9285714285714286"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf.fit(train_attsVar2, train_label[2])\n",
    "prediction = rf.predict(test_attsVar2)\n",
    "metrics.accuracy_score(test_label[2], prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el caso del mejor Accuracy de test, empeoramos dado que perdemos información eliminando las variables de menor importancia, además mejorar una de las mejores configuraciones es más difícil."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Algoritmo de búsqueda recursiva wrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Procedemos a implementar un método wrapper de búsqueda recursiva.\n",
    "En nuestro caso hemos decidido implementar la selección forward, dado que sus características encajan más con nuestras necesidades:\n",
    "- Funciona mejor cuando el subconjunto óptimo tiene pocas variables.\n",
    "- Evalúa conjuntos mucho más pequeños, por lo que es más rápido.\n",
    "- Tiende a evaluar menos subconjuntos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(atts,label,seed):\n",
    "    # Función encargada de retornar un diccionario en el que se eliminan las variables predictoras que no se\n",
    "    # encuentran en la lista \"aSalvar\"\n",
    "    def attsConAtributos(atts,aSalvar):\n",
    "        # Nos guardamos una copia de atts y trabajamos con attsCopy\n",
    "        attsCopy = atts.copy()\n",
    "        # Por cada variable predictora...\n",
    "        for i in atts.keys():\n",
    "            # Comprobaoms si no se encuentra en \"aSalvar\"\n",
    "            # Si no se encuentra, se elimina\n",
    "            if i not in aSalvar:\n",
    "                del attsCopy[i]\n",
    "        return attsCopy\n",
    "    \n",
    "    # Lista que almacena las variables predictoras elegidas\n",
    "    S = []\n",
    "    # Lista con todas las variables predictoras del dataset\n",
    "    X = list(atts.keys())\n",
    "    # Inicializacio de variables, al comienzo, dado que S es vacía, se considera que el score es de 0.\n",
    "    puntuacionS = 0\n",
    "    nuevaPuntuacion = 0\n",
    "    puntuacionSX = 0\n",
    "    # Daremos vueltas hasta que la nueva puntuacion y la puntuacion del sistema sean iguales, o X sea vacía, es\n",
    "    # decir, una vez hallamos comprobado todas las variables predictoras, y todas hayan sido añadidas a S.\n",
    "    while(True):\n",
    "        print(\"S: \")\n",
    "        print(S)\n",
    "        # Por cada variable predictora que nos queda por añadir a S\n",
    "        for i in X:\n",
    "            sCopy = S.copy()\n",
    "            # Siempre se evalua con lo que teníamos en S y la nueva variable predictora (solo una nueva!!!)\n",
    "            sCopy.append(i)\n",
    "            # Obtenemos train_atts con las variables predictoras que tenemos en S y añadimos la que queremos evaluar\n",
    "            # en esta iteración (i)\n",
    "            newAtts = attsConAtributos(atts,sCopy)\n",
    "            # Evaluamos haciendo una validación cruzada\n",
    "            puntuacionSX = cross_val_score(tree.DecisionTreeClassifier(random_state=seed), newAtts , label, cv=3, scoring=\"accuracy\")\n",
    "            puntuacionSX = puntuacionSX.mean()\n",
    "            print(\"Puntuacion\",puntuacionSX,\"incluyendo \",i)\n",
    "            # Si la puntuacion obtenida ha sido mejor que la que ya teniamos, la almacenamos y nos guardamos la \n",
    "            # variable predictora que ha provocado dicha puntuacion\n",
    "            if puntuacionSX>nuevaPuntuacion:\n",
    "                nuevaPuntuacion = puntuacionSX\n",
    "                Xmejor = i\n",
    "        # Tras comprobar las variables predictoras contenidas en X, miramos si la puntuacion que teníamos del\n",
    "        # sistema es peor que la nueva puntuacion, es decir, es peor que si añadiesemos la nueva variable predictora\n",
    "        # , Xmejor\n",
    "        # Si se da el caso, la añadimos a S y la quitamos de X\n",
    "        if puntuacionS<nuevaPuntuacion:\n",
    "            print(\"El mejor ha sido: \" , Xmejor , \"con puntuacion: \", nuevaPuntuacion)\n",
    "            S.append(Xmejor)\n",
    "            X.remove(Xmejor)\n",
    "        # En caso de que la puntuacion de la iteracion anterior haya sido igual que la puntuacion del sistema o se\n",
    "        # hayan añadido todas las variables de X en S, procedemos a salir del bucle.\n",
    "        # Condición de salida:\n",
    "        if nuevaPuntuacion == puntuacionS or len(X)==0:\n",
    "            print(\"No se ha presenciado ninguna variable que mejore el conjunto anterior\")\n",
    "            break\n",
    "        # Si no se ha dado el caso, volvemos a evaluar el sistema con lo contenido hasta ahora en S y \n",
    "        # actualizamos puntuacionS y nuevaPuntuacion, con el fin de tratar en otra iteracion de modificar \n",
    "        # nuevaPuntuacion (puntuacionSX) con un Score mayor que el de puntuacionS\n",
    "        puntuacionS = cross_val_score(tree.DecisionTreeClassifier(random_state=seed), attsConAtributos(atts,S), label, cv=3, scoring=\"accuracy\")\n",
    "        puntuacionS = puntuacionS.mean()\n",
    "        nuevaPuntuacion = puntuacionS\n",
    "    # Se devuelve una lista con las variables que han producido una mejora del Score tras ser añadidas al sistema\n",
    "    return S"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para comprobar el funcionamiento de nuestro algoritmo, vamos a hacer una prueba con el dataset Pima y Wisconsin; y lo compararemos con los atributos seleccionados por el filter de selección de rankings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S: \n",
      "[]\n",
      "Puntuacion 0.680722127039 incluyendo  preg\n",
      "Puntuacion 0.687305666603 incluyendo  plas\n",
      "Puntuacion 0.63681071134 incluyendo  pres\n",
      "Puntuacion 0.626943333968 incluyendo  skin\n",
      "Puntuacion 0.633606193286 incluyendo  insu\n",
      "Puntuacion 0.613950758297 incluyendo  mass\n",
      "Puntuacion 0.573323180405 incluyendo  pedi\n",
      "Puntuacion 0.669268354591 incluyendo  age\n",
      "El mejor ha sido:  plas con puntuacion:  0.687305666603\n",
      "S: \n",
      "['plas']\n",
      "Puntuacion 0.682340250016 incluyendo  preg\n",
      "Puntuacion 0.66777714322 incluyendo  pres\n",
      "Puntuacion 0.695364553588 incluyendo  skin\n",
      "Puntuacion 0.643378386953 incluyendo  insu\n",
      "Puntuacion 0.692334538994 incluyendo  mass\n",
      "Puntuacion 0.675899486008 incluyendo  pedi\n",
      "Puntuacion 0.670965797322 incluyendo  age\n",
      "El mejor ha sido:  skin con puntuacion:  0.695364553588\n",
      "S: \n",
      "['plas', 'skin']\n",
      "Puntuacion 0.659607208579 incluyendo  preg\n",
      "Puntuacion 0.711815470525 incluyendo  pres\n",
      "Puntuacion 0.671076844977 incluyendo  insu\n",
      "Puntuacion 0.67593121391 incluyendo  mass\n",
      "Puntuacion 0.680817310743 incluyendo  pedi\n",
      "Puntuacion 0.667793007171 incluyendo  age\n",
      "El mejor ha sido:  pres con puntuacion:  0.711815470525\n",
      "S: \n",
      "['plas', 'skin', 'pres']\n",
      "Puntuacion 0.677565200838 incluyendo  preg\n",
      "Puntuacion 0.677660384542 incluyendo  insu\n",
      "Puntuacion 0.705263658862 incluyendo  mass\n",
      "Puntuacion 0.682371977917 incluyendo  pedi\n",
      "Puntuacion 0.726457897075 incluyendo  age\n",
      "El mejor ha sido:  age con puntuacion:  0.726457897075\n",
      "S: \n",
      "['plas', 'skin', 'pres', 'age']\n",
      "Puntuacion 0.692239355289 incluyendo  preg\n",
      "Puntuacion 0.708468176915 incluyendo  insu\n",
      "Puntuacion 0.737879941621 incluyendo  mass\n",
      "Puntuacion 0.734485056158 incluyendo  pedi\n",
      "El mejor ha sido:  mass con puntuacion:  0.737879941621\n",
      "S: \n",
      "['plas', 'skin', 'pres', 'age', 'mass']\n",
      "Puntuacion 0.732993844787 incluyendo  preg\n",
      "Puntuacion 0.744336569579 incluyendo  insu\n",
      "Puntuacion 0.744384161432 incluyendo  pedi\n",
      "El mejor ha sido:  pedi con puntuacion:  0.744384161432\n",
      "S: \n",
      "['plas', 'skin', 'pres', 'age', 'mass', 'pedi']\n",
      "Puntuacion 0.729741734882 incluyendo  preg\n",
      "Puntuacion 0.729598959325 incluyendo  insu\n",
      "No se ha presenciado ninguna variable que mejore el conjunto anterior\n",
      "Variables con mejor Accuracy - Nuestro algoritmo: \n",
      "['plas', 'skin', 'pres', 'age', 'mass', 'pedi']\n"
     ]
    }
   ],
   "source": [
    "var = forward(train_atts[1],train_label[1],seed)\n",
    "print(\"Variables con mejor Accuracy - Nuestro algoritmo: \")\n",
    "print(var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resultado obtenido con filter:\n",
    "- (0.1288987268794517, 'plas'),\n",
    "- (0.094889792970892284, 'mass'),\n",
    "- (0.067196685611855322, 'age'),\n",
    "- (0.064457008631584101, 'preg'),\n",
    "- (0.040198824529407595, 'pres'),\n",
    "- (0.035925099264167093, 'insu'),\n",
    "- (0.014890438929848759, 'skin'),\n",
    "- (0.0, 'pedi')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos ver, que más o menos las variables obtenidas son similares, siendo la más importante plas. Aunque hay que tener en cuenta, que nuestro algoritmo hace una combinación de todas las variables y es por ello que obtenemos que hay variables más importantes junto con otras, y sin embargo filter las analiza de manera individual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S: \n",
      "[]\n",
      "Puntuacion 0.858672110095 incluyendo  clumpThickness\n",
      "Puntuacion 0.932005060089 incluyendo  cellSize\n",
      "Puntuacion 0.930222528894 incluyendo  CellShape\n",
      "Puntuacion 0.844421444042 incluyendo  marginalAdhesion\n",
      "Puntuacion 0.880196653441 incluyendo  epithelialSize\n",
      "Puntuacion 0.889128476415 incluyendo  bareNuclei\n",
      "Puntuacion 0.910557184751 incluyendo  blandChromatin\n",
      "Puntuacion 0.896287351695 incluyendo  normalNucleoli\n",
      "Puntuacion 0.799599409656 incluyendo  mitoses\n",
      "El mejor ha sido:  cellSize con puntuacion:  0.932005060089\n",
      "S: \n",
      "['cellSize']\n",
      "Puntuacion 0.926657466506 incluyendo  clumpThickness\n",
      "Puntuacion 0.928459164702 incluyendo  CellShape\n",
      "Puntuacion 0.928459164702 incluyendo  marginalAdhesion\n",
      "Puntuacion 0.944540279455 incluyendo  epithelialSize\n",
      "Puntuacion 0.948114925345 incluyendo  bareNuclei\n",
      "Puntuacion 0.923063653614 incluyendo  blandChromatin\n",
      "Puntuacion 0.942757748261 incluyendo  normalNucleoli\n",
      "Puntuacion 0.930203361892 incluyendo  mitoses\n",
      "El mejor ha sido:  bareNuclei con puntuacion:  0.948114925345\n",
      "S: \n",
      "['cellSize', 'bareNuclei']\n",
      "Puntuacion 0.948105341843 incluyendo  clumpThickness\n",
      "Puntuacion 0.944540279455 incluyendo  CellShape\n",
      "Puntuacion 0.951737488739 incluyendo  marginalAdhesion\n",
      "Puntuacion 0.942776915263 incluyendo  epithelialSize\n",
      "Puntuacion 0.926638299504 incluyendo  blandChromatin\n",
      "Puntuacion 0.942767331762 incluyendo  normalNucleoli\n",
      "Puntuacion 0.940936883062 incluyendo  mitoses\n",
      "El mejor ha sido:  marginalAdhesion con puntuacion:  0.951737488739\n",
      "S: \n",
      "['cellSize', 'bareNuclei', 'marginalAdhesion']\n",
      "Puntuacion 0.951699154735 incluyendo  clumpThickness\n",
      "Puntuacion 0.948124508846 incluyendo  CellShape\n",
      "Puntuacion 0.953510436433 incluyendo  epithelialSize\n",
      "Puntuacion 0.946361144653 incluyendo  blandChromatin\n",
      "Puntuacion 0.949916623541 incluyendo  normalNucleoli\n",
      "Puntuacion 0.95348168593 incluyendo  mitoses\n",
      "El mejor ha sido:  epithelialSize con puntuacion:  0.953510436433\n",
      "S: \n",
      "['cellSize', 'bareNuclei', 'marginalAdhesion', 'epithelialSize']\n",
      "Puntuacion 0.946351561152 incluyendo  clumpThickness\n",
      "Puntuacion 0.955273800625 incluyendo  CellShape\n",
      "Puntuacion 0.944549862956 incluyendo  blandChromatin\n",
      "Puntuacion 0.95706591532 incluyendo  normalNucleoli\n",
      "Puntuacion 0.955292967627 incluyendo  mitoses\n",
      "El mejor ha sido:  normalNucleoli con puntuacion:  0.95706591532\n",
      "S: \n",
      "['cellSize', 'bareNuclei', 'marginalAdhesion', 'epithelialSize', 'normalNucleoli']\n",
      "Puntuacion 0.949945374044 incluyendo  clumpThickness\n",
      "Puntuacion 0.951718321737 incluyendo  CellShape\n",
      "Puntuacion 0.960669311713 incluyendo  blandChromatin\n",
      "Puntuacion 0.949916623541 incluyendo  mitoses\n",
      "El mejor ha sido:  blandChromatin con puntuacion:  0.960669311713\n",
      "S: \n",
      "['cellSize', 'bareNuclei', 'marginalAdhesion', 'epithelialSize', 'normalNucleoli', 'blandChromatin']\n",
      "Puntuacion 0.955283384126 incluyendo  clumpThickness\n",
      "Puntuacion 0.946341977651 incluyendo  CellShape\n",
      "Puntuacion 0.948114925345 incluyendo  mitoses\n",
      "No se ha presenciado ninguna variable que mejore el conjunto anterior\n",
      "Variables con mejor Accuracy - Nuestro algoritmo: \n",
      "['cellSize', 'bareNuclei', 'marginalAdhesion', 'epithelialSize', 'normalNucleoli', 'blandChromatin']\n"
     ]
    }
   ],
   "source": [
    "var = forward(train_atts[2],train_label[2],seed)\n",
    "print(\"Variables con mejor Accuracy - Nuestro algoritmo: \")\n",
    "print(var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resultado obtenido con filter:\n",
    "- (0.48603098768879649, 'cellSize'),\n",
    "- (0.46993766833372019, 'CellShape'),\n",
    "- (0.41167880380619892, 'bareNuclei'),\n",
    "- (0.37911994505258395, 'blandChromatin'),\n",
    "- (0.35302212105383335, 'epithelialSize'),\n",
    "- (0.31501779705255006, 'clumpThickness'),\n",
    "- (0.31169836388044914, 'normalNucleoli'),\n",
    "- (0.30767797478207859, 'marginalAdhesion'),\n",
    "- (0.14952150918842766, 'mitoses')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como podemos ver, la explicación es similar, se eligen variables muy similares, han elegido de igual manera las variables más importantes \"cellSize\",\"Cellshape\" y las menos importantes como \"mitoses\" las desprecia."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
